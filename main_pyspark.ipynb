{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830e16a7-c3c9-49b1-8ee0-ed389eca2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a sparksession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea519d22-e5b1-4480-9fb9-3fbebd9a565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sparksession and store in spark variable\n",
    "spark=SparkSession.builder \\\n",
    ".appName(\"test\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727093bc-9075-41c6-8096-203ac1c81941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a df with 10 numbers\n",
    "df=spark.range(10).toDF(\"numbers\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3833e5b-69f9-498b-a07a-75184ec5641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      2|\n",
      "|      4|\n",
      "|      6|\n",
      "|      8|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from df take only divisibile_by_2 numbers and store in divisibile_by_2 variable\n",
    "divisible_by_2=df.where(\"numbers%2=0\")\n",
    "divisible_by_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934287fe-cd74-4445-9ef1-ff46393fad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and show the output\n",
    "csv_load=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "csv_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acef7703-08bd-4a75-99fb-9e62ff9968fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Estonia|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|              Zambia|      United States|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "|       United States|           Bulgaria|    1|\n",
      "|       United States|            Georgia|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#33 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#33 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=65]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#31,ORIGIN_COUNTRY_NAME#32,count#33] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and sort the data with count column and show the output\n",
    "from pyspark.sql.functions import desc,asc\n",
    "a=csv_load.sort(asc(\"count\"))\n",
    "a.show()\n",
    "a.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f05d5c-1f15-4775-b213-f4569c850802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\"\n",
    "csv_load.createOrReplaceTempView(\"sql_csv_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38c1ef9-09de-4a76-a54a-4d2f1420d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|            Anguilla|       1|\n",
      "|              Russia|       1|\n",
      "|            Paraguay|       1|\n",
      "|             Senegal|       1|\n",
      "|              Sweden|       1|\n",
      "|            Kiribati|       1|\n",
      "|              Guyana|       1|\n",
      "|         Philippines|       1|\n",
      "|            Djibouti|       1|\n",
      "|            Malaysia|       1|\n",
      "|           Singapore|       1|\n",
      "|                Fiji|       1|\n",
      "|              Turkey|       1|\n",
      "|                Iraq|       1|\n",
      "|             Germany|       1|\n",
      "|              Jordan|       1|\n",
      "|               Palau|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|              France|       1|\n",
      "|              Greece|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\",\n",
    "#write a query in sql with grouping DEST_COUNTRY_NAME having count(1),stroing the data in sql_data variable\n",
    "sql_data=spark.sql(\"select DEST_COUNTRY_NAME,count(*) from sql_csv_table group by DEST_COUNTRY_NAME\")\n",
    "sql_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5386e7-ed1c-41e1-ae54-75c7d85d4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\",\n",
    "# write a query in sql with max count column,stroing the data in sql_data variable\n",
    "#==========\n",
    "#load the csv file and store it in csv_load variable, select max of the count column store the result in pyspark_sql variable\n",
    "\n",
    "sql_data=spark.sql(\"select max(count) from sql_csv_table\")\n",
    "sql_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "633dd7e4-62d8-444a-9fad-1d9066008dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "| DEST_COUNTRY_NAME|destination|\n",
      "+------------------+-----------+\n",
      "|     United States|     411352|\n",
      "|            Canada|       8399|\n",
      "|            Mexico|       7140|\n",
      "|    United Kingdom|       2025|\n",
      "|             Japan|       1548|\n",
      "|           Germany|       1468|\n",
      "|Dominican Republic|       1353|\n",
      "|       South Korea|       1048|\n",
      "|       The Bahamas|        955|\n",
      "|            France|        935|\n",
      "|          Colombia|        873|\n",
      "|            Brazil|        853|\n",
      "|       Netherlands|        776|\n",
      "|             China|        772|\n",
      "|           Jamaica|        666|\n",
      "|        Costa Rica|        588|\n",
      "|       El Salvador|        561|\n",
      "|            Panama|        510|\n",
      "|              Cuba|        466|\n",
      "|             Spain|        420|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['destination DESC NULLS LAST], true\n",
      "+- 'Aggregate ['DEST_COUNTRY_NAME], ['DEST_COUNTRY_NAME, 'sum('count) AS destination#99]\n",
      "   +- 'UnresolvedRelation [sql_csv_table], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "DEST_COUNTRY_NAME: string, destination: bigint\n",
      "Sort [destination#99L DESC NULLS LAST], true\n",
      "+- Aggregate [DEST_COUNTRY_NAME#31], [DEST_COUNTRY_NAME#31, sum(count#33) AS destination#99L]\n",
      "   +- SubqueryAlias sql_csv_table\n",
      "      +- View (`sql_csv_table`, [DEST_COUNTRY_NAME#31,ORIGIN_COUNTRY_NAME#32,count#33])\n",
      "         +- Relation [DEST_COUNTRY_NAME#31,ORIGIN_COUNTRY_NAME#32,count#33] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [destination#99L DESC NULLS LAST], true\n",
      "+- Aggregate [DEST_COUNTRY_NAME#31], [DEST_COUNTRY_NAME#31, sum(count#33) AS destination#99L]\n",
      "   +- Project [DEST_COUNTRY_NAME#31, count#33]\n",
      "      +- Relation [DEST_COUNTRY_NAME#31,ORIGIN_COUNTRY_NAME#32,count#33] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [destination#99L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(destination#99L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=210]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#31], functions=[sum(count#33)], output=[DEST_COUNTRY_NAME#31, destination#99L])\n",
      "         +- Exchange hashpartitioning(DEST_COUNTRY_NAME#31, 200), ENSURE_REQUIREMENTS, [plan_id=207]\n",
      "            +- HashAggregate(keys=[DEST_COUNTRY_NAME#31], functions=[partial_sum(count#33)], output=[DEST_COUNTRY_NAME#31, sum#110L])\n",
      "               +- FileScan csv [DEST_COUNTRY_NAME#31,count#33] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write a query in sql selecting DEST_COUNTRY_NAME and sum of count of grouping DEST_COUNTRY_NAME as destination,show the result in desc\n",
    "#store the result in csv_sql variable\n",
    "csv_sql=spark.sql(\"select DEST_COUNTRY_NAME,sum(count) as destination from sql_csv_table group by DEST_COUNTRY_NAME order by destination desc\")\n",
    "csv_sql.show()\n",
    "csv_sql.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1cacbd4-0420-4bf2-96a8-d3e887598338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "| DEST_COUNTRY_NAME|destination|\n",
      "+------------------+-----------+\n",
      "|     United States|     411352|\n",
      "|            Canada|       8399|\n",
      "|            Mexico|       7140|\n",
      "|    United Kingdom|       2025|\n",
      "|             Japan|       1548|\n",
      "|           Germany|       1468|\n",
      "|Dominican Republic|       1353|\n",
      "|       South Korea|       1048|\n",
      "|       The Bahamas|        955|\n",
      "|            France|        935|\n",
      "|          Colombia|        873|\n",
      "|            Brazil|        853|\n",
      "|       Netherlands|        776|\n",
      "|             China|        772|\n",
      "|           Jamaica|        666|\n",
      "|        Costa Rica|        588|\n",
      "|       El Salvador|        561|\n",
      "|            Panama|        510|\n",
      "|              Cuba|        466|\n",
      "|             Spain|        420|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable\n",
    "#selecting DEST_COUNTRY_NAME and sum of count of grouping DEST_COUNTRY_NAME as destination,show the result in desc\n",
    "from pyspark.sql.functions import sum,col\n",
    "a=csv_load.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\",\"destination\").sort(desc(\"destination\"))\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8263edd5-b799-4d3b-a7ee-932b7841282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+------------------------------------------------------------------+\n",
      "|InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |filename                                                          |\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+------------------------------------------------------------------+\n",
      "|580538   |23084    |RABBIT NIGHT LIGHT             |48      |2011-12-05 08:38:00|1.79     |14075.0   |United Kingdom|file:///home/jovyan/directory%20to%20extract/by-day/2011-12-05.csv|\n",
      "|580538   |23077    |DOUGHNUT LIP GLOSS             |20      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|file:///home/jovyan/directory%20to%20extract/by-day/2011-12-05.csv|\n",
      "|580538   |22906    |12 MESSAGE CARDS WITH ENVELOPES|24      |2011-12-05 08:38:00|1.65     |14075.0   |United Kingdom|file:///home/jovyan/directory%20to%20extract/by-day/2011-12-05.csv|\n",
      "|580538   |21914    |BLUE HARMONICA IN BOX          |24      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|file:///home/jovyan/directory%20to%20extract/by-day/2011-12-05.csv|\n",
      "|580538   |22467    |GUMBALL COAT RACK              |6       |2011-12-05 08:38:00|2.55     |14075.0   |United Kingdom|file:///home/jovyan/directory%20to%20extract/by-day/2011-12-05.csv|\n",
      "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the by_day folder and store in load_byday_csv and create the sql temp table as retail_data, assign the schema of load_byday_csv to schema\n",
    "#and print the schema\n",
    "from pyspark.sql.functions import input_file_name\n",
    "load_byday_csv=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"directory to extract/by-day/*.csv\")\n",
    "load_byday_csv.createOrReplaceTempView(\"retail_data\")\n",
    "schema=load_byday_csv.schema\n",
    "load_byday_csv.show()\n",
    "\n",
    "load_byday_csv1=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"directory to extract/by-day\").withColumn(\"filename\",input_file_name())\n",
    "load_byday_csv1.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "350a4d74-f90e-4baf-bb99-dc219439d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+------------------+\n",
      "|CustomerID|window                                    |total_cost        |\n",
      "+----------+------------------------------------------+------------------+\n",
      "|17450.0   |{2011-09-20 00:00:00, 2011-09-21 00:00:00}|71601.44          |\n",
      "|NULL      |{2011-11-14 00:00:00, 2011-11-15 00:00:00}|55316.08          |\n",
      "|NULL      |{2011-11-07 00:00:00, 2011-11-08 00:00:00}|42939.17          |\n",
      "|NULL      |{2011-03-29 00:00:00, 2011-03-30 00:00:00}|33521.39999999998 |\n",
      "|NULL      |{2011-12-08 00:00:00, 2011-12-09 00:00:00}|31975.590000000007|\n",
      "+----------+------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the by_day folder and store in load_byday_csv and select CustomerID,UnitPrice * Quantity as total_cost,InvoiceDate\n",
    "#group by CustomerID with window function as 1 day, sum the total and rename it to total_cost and sort with desc of total_cost\n",
    "from pyspark.sql.functions import desc,asc,window,col\n",
    "a=load_byday_csv.selectExpr(\"CustomerID\",\"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\") \\\n",
    "    .groupBy(\"CustomerID\",window(col(\"InvoiceDate\"),\"1 day\")).sum(\"total_cost\") \\\n",
    "    .withColumnRenamed(\"sum(total_cost)\",\"total_cost\").sort(desc(\"total_cost\"))\n",
    "a.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d440088-9ed2-4bed-b6bc-fbee33e4b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually set the shuffle partitions to 5\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e76b17-328d-49e5-b1b1-2bcd894e7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv files and store in load_by_data variable, store the schema in schema variable\n",
    "#read the stream by setting maxFilesPerTrigger as 1\n",
    "streaming_data=spark.readStream.schema(schema).option(\"header\",\"true\").option(\"maxFilesPerTrigger\",1) \\\n",
    "                .load(\"directory to extract/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8baad8-4490-4b67-98bc-17b6ab70cc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if its streaming or not\n",
    "streaming_data.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e7b9d7-93da-4fcd-af07-ebeafee12fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv files and store in load_by_data variable, store the schema in schema variable\n",
    "#read the stream by setting maxFilesPerTrigger as 1 and check if its streaming or not in read_stream variable\n",
    "#with read_stream select CustomerID,UnitPrice * Quantity as total_cost,InvoiceDate\n",
    "#group by CustomerID with window function as 1 day, sum the total_cost store the entire result in purchasebycustomerperhour\n",
    "from pyspark.sql.functions import col,window\n",
    "load_by_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"directory to extract/by-day/*\")\n",
    "schema=load_by_data.schema\n",
    "read_stream=spark.readStream.option(\"maxFilesPerTrigger\",1).schema(schema).option(\"header\",\"true\").csv(\"directory to extract/by-day/*\")\n",
    "purchasebycustomerperhour=read_stream.selectExpr(\"CustomerID\",\"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\") \\\n",
    "                        .groupBy(\"CustomerID\",window(col(\"InvoiceDate\"),\"1 day\")).sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb4e2b9-fa9a-4872-9164-5721c9e2dc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7ff3b0c96050>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop the write_stream and with purchasebycustomerperhour write the stream,store in memory,and tablename is customer_purchases, output the mode as complete and start\n",
    "purchasebycustomerperhour.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badc885b-3276-48f8-aeae-40b0463ec76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerID|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select all the columns with sql in customer_purchases table and store in customer variable\n",
    "customer=spark.sql(\"select * from customer_purchases\")\n",
    "customer.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b97992-60d8-47da-8cba-55f88585db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', IntegerType(), True), StructField('InvoiceDate', TimestampType(), True), StructField('UnitPrice', DoubleType(), True), StructField('CustomerID', DoubleType(), True), StructField('Country', StringType(), True)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the csv files directory to extract/by-day/* and store in load_by_data variable and print the schema in 2 different ways\n",
    "load_by_data.printSchema()\n",
    "load_by_data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cee414d-f14c-4073-975a-66dedb8fbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import date_format and col functions\n",
    "#in preppedDataFrame fill null values with 0 and add new column day_of_week contains the format of \"EEEE\" to column InvoiceDate\n",
    "#and reduce the partitions to 5\n",
    "from pyspark.sql.functions import date_format,col\n",
    "preppedDataFrame=load_by_data.na.fill(0).withColumn(\"day_of_week\",date_format(col(\"InvoiceDate\"),\"EEEE\")).coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "565af968-4620-4881-ad33-03a9e38b9333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296006"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using preppedDataFrame create 2 variables\n",
    "#one contains preppedDataFrame of InvoiceDate < '2011-07-01' and store in trainDataFrame varibale\n",
    "#another contains preppedDataFrame of InvoiceDate >= '2011-07-01' and store in testDataFrame varibale\n",
    "#check the count of trainDataFrame,testDataFrame\n",
    "\n",
    "trainDataFrame=preppedDataFrame.where(\"InvoiceDate<'2011-07-01'\")\n",
    "testDataFrame=preppedDataFrame.where(\"InvoiceDate>='2011-07-01'\")\n",
    "trainDataFrame.count() #245903\n",
    "testDataFrame.count() #296006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e04680d2-460c-4cee-9fbb-33a1057b6f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe in rdd_df variable contians 1,2,3 using row\n",
    "from pyspark.sql import Row\n",
    "rdd_df=spark.sparkContext.parallelize([Row(1),Row(2),Row(3)]).toDF()\n",
    "rdd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0748c84e-503f-4806-914e-e2e5e28ab174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|numbers2|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+\n",
      "|(numbers2 + 10)|\n",
      "+---------------+\n",
      "|             10|\n",
      "|             11|\n",
      "|             12|\n",
      "|             13|\n",
      "|             14|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe number_500 contains 500 numbers and add each value + 10 and show the result\n",
    "number_500=spark.range(500).toDF(\"numbers2\")\n",
    "number_500.show(5)\n",
    "number_500.select(number_500[\"numbers2\"]+10).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8e07d2-9e1b-454f-a709-0e4d6f5a7227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(new_df=0), Row(new_df=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#range of 2 numbers collect the numbers and store in rows_collect and print rows_collect\n",
    "rows_collect=spark.range(2).toDF(\"new_df\")\n",
    "rows_collect.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61414ae4-07de-4aba-91ba-6997a3d7d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data and store in json_data variable and show the result\n",
    "json_data=spark.read.format(\"json\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").json(\"2015-summary.json\")\n",
    "json_data.show(5)\n",
    "json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6eff8650-262b-4352-8806-e86e61262e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#manually assign schema of to the json_data by loading and select only count column and store the result in a \n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType\n",
    "schema=StructType([StructField(\"DEST_COUNTRY_NAME\",StringType(),True),StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),StructField(\"count\",LongType(),True)])\n",
    "a=spark.read.format(\"json\").schema(schema).option(\"header\",\"true\").load(\"2015-summary.json\")\n",
    "a.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc204481-6b89-4dfe-98e8-d37a65fffe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#manually assign schema of to the json_data by loading and select only count column and store the result in a \n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType\n",
    "schema=StructType([StructField(\"DEST_COUNTRY_NAME\",StringType(),True),StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),StructField(\"count\",LongType(),True)])\n",
    "a=spark.read.format(\"json\").schema(schema).load(\"2015-summary.json\")\n",
    "a.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31a83852-1ee3-4ede-a480-ae32e77fe34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "a.columns\n",
    "a.first()\n",
    "a.select(col(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8739030e-50db-4f0f-bbe6-4b4192d81cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n"
     ]
    }
   ],
   "source": [
    "#access only columns of json data and store in json_data and print the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\").columns\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7dea1d7-8044-4d8f-ad89-c875f870c323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('dolly', 2)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a row of values manually and show the result of 2nd index\n",
    "from pyspark.sql import Row\n",
    "manual_rows=([Row(\"name\",\"no\"),Row(\"sush\",1),Row(\"dolly\",2)])\n",
    "manual_rows[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99222d54-a35e-4d8e-aeb6-f4183fcbc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the json data into json_data variable and create temp table df_table\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c39c6f6d-ef52-4395-bb9b-7810d4091485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n",
      "|user_name|    city|num|\n",
      "+---------+--------+---+\n",
      "|     sush|Kompally|  1|\n",
      "|    dolly|     kmr|  2|\n",
      "+---------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in manual_schema store schema(string,string,int) and df_data store random data \n",
    "#and create the dataframe in new_df variable with df_data and manual_schema and print the result of new_df\n",
    "from pyspark.sql.types import StructField,StructType,StringType,LongType\n",
    "manual_schema=StructType([StructField(\"user_name\",StringType(),False),StructField(\"city\",StringType(),False),StructField(\"num\",LongType(),False)])\n",
    "df_data=[Row(\"sush\",\"Kompally\",1),Row(\"dolly\",\"kmr\",2)]\n",
    "new_df=spark.createDataFrame(df_data,manual_schema)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b75c50b4-15c6-4934-9e1a-9685d5f31d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "|            Egypt|      United States|\n",
      "|    United States|              India|\n",
      "+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select only \"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\" and show the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(col(\"DEST_COUNTRY_NAME\"),col(\"ORIGIN_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b2f9554-5076-4d27-a5b0-e19a1ea8a837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and using col,column,expr select the column DEST_COUNTRY_NAME and show the result of 2 records\n",
    "from pyspark.sql.functions import col,column,expr\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(col(\"DEST_COUNTRY_NAME\"),column(\"DEST_COUNTRY_NAME\"),expr(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c56fcea3-b575-4624-ba11-97e26113e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select the column DEST_COUNTRY_NAME as destination with expr and show 2 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(col(\"DEST_COUNTRY_NAME\").alias(\"destination\")).show(2)\n",
    "json_data.select(expr(\"DEST_COUNTRY_NAME\").alias(\"destination\")).show(2)\n",
    "json_data.select(expr(\"DEST_COUNTRY_NAME as destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9365c0f-6016-4f78-8bc7-d62f47868ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| destination1|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "|        Egypt|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select the column DEST_COUNTRY_NAME as dest with expr, again rename to destination1 and show 5 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(expr(\"DEST_COUNTRY_NAME as dest\").alias(\"destination1\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b52fdf33-2cf2-4718-b3de-8778a8eb0a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|         dest|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select the column DEST_COUNTRY_NAME as dest, DEST_COUNTRY_NAME with selectExpr, show 2 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.selectExpr(\"DEST_COUNTRY_NAME as dest\",\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "311ef3db-4053-4fb4-bca3-15eae47aa964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select all columns, if DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME store in withincountry and show 5 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.selectExpr(\"*\",\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(5)\n",
    "\n",
    "json_data.withColumn(\"withinCountry\",(col(\"DEST_COUNTRY_NAME\") == col(\"ORIGIN_COUNTRY_NAME\"))).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2826487c-447f-419a-b4a9-4597858e8225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select avg of count,count of distinct DEST_COUNTRY_NAME\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.selectExpr(\"avg(count)\",\"count(distinct DEST_COUNTRY_NAME)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "400a9525-7045-459e-b1ff-9e0efb36ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select all columns of json_data and add new column as One stroing lit value of 1 and print 2 records\n",
    "from pyspark.sql.functions import lit\n",
    "json_data.withColumn(\"one\",lit(1)).show(2)\n",
    "json_data.select(expr(\"*\"),lit(1).alias(\"one\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c023e391-f926-49ab-a61f-dfc52f945d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|         1|\n",
      "|    United States|            Croatia|    1|         1|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|     false|\n",
      "|    United States|            Croatia|    1|     false|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|  destination|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|United States|\n",
      "|    United States|            Croatia|    1|United States|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+-------------------+-----+\n",
      "|  destination|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#a variable contains data of adding new column name as new_column contains value of literal 1 and show 2 records\n",
    "#b variable contains data of adding new column name as new_column contains value if DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME and show 2 records\n",
    "#c variable contains data of adding new column name as destination contains value of DEST_COUNTRY_NAME and show 2 records\n",
    "#d variable contains data of renaming column DEST_COUNTRY_NAME to new_column destination and show 2 records\n",
    "a=json_data.withColumn(\"new_column\",lit(1))\n",
    "a.show(2)\n",
    "b=json_data.withColumn(\"new_column\",col(\"DEST_COUNTRY_NAME\") == col(\"ORIGIN_COUNTRY_NAME\"))\n",
    "b.show(2)\n",
    "c=json_data.withColumn(\"destination\",col(\"DEST_COUNTRY_NAME\"))\n",
    "c.show(2)\n",
    "d=json_data.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"destination\")\n",
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cb42fb4-f784-430b-ac01-c8643e0d0fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------+----------+\n",
      "|This Long Column-name|new_column|\n",
      "+---------------------+----------+\n",
      "|              Romania|   Romania|\n",
      "|              Croatia|   Croatia|\n",
      "+---------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "['This Long Column-name']\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#a variable contains data of adding new column name as \"This Long Column-name\" contains value of ORIGIN_COUNTRY_NAME and show 2 records\n",
    "#b variable contains data of selecting new column name as \"This Long Column-name\" and selecting \"This Long Column-name\" as new_column and show 2 records\n",
    "#c variable contains data of selecting new column name as \"This Long Column-name\" and columns and print the result\n",
    "a=json_data.withColumn(\"This Long Column-name\",col(\"ORIGIN_COUNTRY_NAME\"))\n",
    "a.show(2)\n",
    "b=a.select(\"This Long Column-name\",col(\"This Long Column-name\").alias(\"new_column\"))\n",
    "b.show(2)\n",
    "c=b.select(col(\"new_column\").alias(\"This Long Column-name\")).columns\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "759e782d-dc38-4a69-b2e3-effbff9060dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and delete the column DEST_COUNTRY_NAME and show 2 records\n",
    "json_data.drop(col(\"DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d85f03c2-2045-4a2a-ace7-35f54d719662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|        15|\n",
      "|    United States|            Croatia|    1|         1|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable and \n",
    "#a variable contains data of adding new column name as \"new_column\" contains value of \"count\" column and show 2 records,schema\n",
    "#b variable contains data of a and selecting new_column column name and change the type to int and show the schema\n",
    "a=json_data.withColumn(\"new_column\",col(\"count\"))\n",
    "a.show(2)\n",
    "a.schema\n",
    "b=a.withColumn(\"new_column\",col(\"count\").cast(\"int\"))\n",
    "b.schema\n",
    "\n",
    "c=a.select(col(\"count\").cast(\"int\"))\n",
    "c.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7daef7af-f597-435d-8c07-b1cabac65c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "256\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and \n",
    "#a variable contains data of count<2 with where condition and show 4 records\n",
    "#b variable contains data of count<2 with filter condition and show 4 records\n",
    "#c variable contains data of count<2,ORIGIN_COUNTRY_NAME!=Croatia with where condition and show 4 records\n",
    "#d variable contains data of \"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\" distinct and show count of records\n",
    "#e variable contains data of \"DEST_COUNTRY_NAME\" distinct and show count of records\n",
    "\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.where(col(\"count\")<2)\n",
    "a.show(4)\n",
    "b=json_data.filter(col(\"count\")<2)\n",
    "b.show(4)\n",
    "c=json_data.where(col(\"count\")<2).where(col(\"ORIGIN_COUNTRY_NAME\")!=\"Croatia\")\n",
    "c.show(4)\n",
    "d=json_data.select(col(\"DEST_COUNTRY_NAME\"),col(\"ORIGIN_COUNTRY_NAME\")).distinct().count()\n",
    "print(d)\n",
    "e=json_data.select(col(\"DEST_COUNTRY_NAME\")).distinct().count()\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d0ee34d-d624-4c6c-b5f5-3a5b303fb4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable, with sample split the data into 50% with constant 5 with replacement as false and count the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "seed=5\n",
    "withReplacement=False\n",
    "fraction=0.5\n",
    "json_data.sample(withReplacement,fraction,seed).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a32d4a27-6fb3-4aeb-9072-4fbb142ee027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 185\n"
     ]
    }
   ],
   "source": [
    "#import the json data in json_data variable and split the data into 25%,75% with seed 5 and check count if its true or false\n",
    "a=json_data.randomSplit([0.25,0.75],5)\n",
    "print(a[0].count(),a[1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff50c0ec-4da8-4f9e-9bb1-b941e6f0d996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Liberia|      United States|    2|\n",
      "|          Hungary|      United States|    2|\n",
      "|    United States|            Vietnam|    2|\n",
      "|         Malaysia|      United States|    2|\n",
      "|          Croatia|      United States|    2|\n",
      "|    United States|            Liberia|    2|\n",
      "|    United States|              Malta|    2|\n",
      "|          Georgia|      United States|    2|\n",
      "|            Niger|      United States|    2|\n",
      "|    United States|          Indonesia|    2|\n",
      "|        Greenland|      United States|    2|\n",
      "|             sush|              asara|    2|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data into json_data variable and store the schema of json_data in schema variable\n",
    "#add new_rows contains (\"sush\",\"asara\",2),(\"dolly\",\"asara\",4)\n",
    "#parallelize the new_rows store in parallelize variable\n",
    "#create a dataframe with parallelize ,schema in new_df variable and show the result of new_df\n",
    "#json_data union new_df where count=2,dest_country_name!=origin_country_name and show the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "schema=json_data.schema\n",
    "new_rows=([Row(\"sush\",\"asara\",2),Row(\"dolly\",\"asara\",4)])\n",
    "parallelize=spark.sparkContext.parallelize(new_rows)\n",
    "new_df=spark.createDataFrame(new_rows,schema)\n",
    "json_data.union(new_df).where(col(\"count\")==2).where(\"dest_country_name!=origin_country_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d85e5969-74d0-48c9-b043-39d842892e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|           Indonesia|      United States|    1|\n",
      "|                Iraq|      United States|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|       New Caledonia|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|            Estonia|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#in a variable sort the result of count with sort and show 5 records\n",
    "#in b variable sort the result of count,DEST_COUNTRY_NAME with orderBy and show 5 records\n",
    "a=json_data.sort(\"count\")\n",
    "a.show(5)\n",
    "b=json_data.orderBy(\"count\",\"DEST_COUNTRY_NAME\")\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5dc506b-61fd-4d38-b592-68ec6c7d9282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "|           Angola|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#sort the count values in desc with orderby in a variable and show 2 records\n",
    "#sort the count values in desc and dest_country_name in asc with orderby in b variable and show 2 records\n",
    "from pyspark.sql.functions import asc,desc\n",
    "a=json_data.orderBy(desc(\"count\"))\n",
    "a.show(2)\n",
    "b=json_data.orderBy(desc(\"count\"),asc(\"dest_country_name\"))\n",
    "b=json_data.orderBy(asc(\"dest_country_name\"),desc(\"count\"))\n",
    "b.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87f27885-8b07-416e-9bce-474d36a5b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable and using limit with 5 show the result\n",
    "#in a variable sort the count desc and using limit with 5 show the result\n",
    "#in b variable repartition json_data to 5 and check how many partitioned it got\n",
    "#in c variable repartition json_data based on column ORIGIN_COUNTRY_NAME and check how many partitioned it got\n",
    "#in d variable repartition json_data based on column DEST_COUNTRY_NAME with 4 and check how many partitioned it got\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.show(5)\n",
    "a=json_data.sort(desc(\"count\")).limit(5)\n",
    "a.show()\n",
    "b=json_data.repartition(5)\n",
    "b.rdd.getNumPartitions()\n",
    "c=json_data.repartition(col(\"ORIGIN_COUNTRY_NAME\"))\n",
    "c.rdd.getNumPartitions()\n",
    "d=json_data.repartition(4,col(\"DEST_COUNTRY_NAME\"))\n",
    "d.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc3b3a81-f2a9-44fb-b173-e9db841b5c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Sint Maarten', count=325),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Marshall Islands', count=39),\n",
       " Row(DEST_COUNTRY_NAME='Guyana', ORIGIN_COUNTRY_NAME='United States', count=64),\n",
       " Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Anguilla', ORIGIN_COUNTRY_NAME='United States', count=41),\n",
       " Row(DEST_COUNTRY_NAME='Bolivia', ORIGIN_COUNTRY_NAME='United States', count=30),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Paraguay', count=6),\n",
       " Row(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4),\n",
       " Row(DEST_COUNTRY_NAME='Turks and Caicos Islands', ORIGIN_COUNTRY_NAME='United States', count=230),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Italy', ORIGIN_COUNTRY_NAME='United States', count=382),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Federated States of Micronesia', count=69),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Russia', count=161),\n",
       " Row(DEST_COUNTRY_NAME='Pakistan', ORIGIN_COUNTRY_NAME='United States', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Netherlands', count=660),\n",
       " Row(DEST_COUNTRY_NAME='Iceland', ORIGIN_COUNTRY_NAME='United States', count=181),\n",
       " Row(DEST_COUNTRY_NAME='Marshall Islands', ORIGIN_COUNTRY_NAME='United States', count=42),\n",
       " Row(DEST_COUNTRY_NAME='Luxembourg', ORIGIN_COUNTRY_NAME='United States', count=155),\n",
       " Row(DEST_COUNTRY_NAME='Honduras', ORIGIN_COUNTRY_NAME='United States', count=362),\n",
       " Row(DEST_COUNTRY_NAME='The Bahamas', ORIGIN_COUNTRY_NAME='United States', count=955),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Senegal', count=42),\n",
       " Row(DEST_COUNTRY_NAME='El Salvador', ORIGIN_COUNTRY_NAME='United States', count=561),\n",
       " Row(DEST_COUNTRY_NAME='Samoa', ORIGIN_COUNTRY_NAME='United States', count=25),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Angola', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Switzerland', ORIGIN_COUNTRY_NAME='United States', count=294),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Anguilla', count=38),\n",
       " Row(DEST_COUNTRY_NAME='Sint Maarten', ORIGIN_COUNTRY_NAME='United States', count=325),\n",
       " Row(DEST_COUNTRY_NAME='Hong Kong', ORIGIN_COUNTRY_NAME='United States', count=332),\n",
       " Row(DEST_COUNTRY_NAME='Trinidad and Tobago', ORIGIN_COUNTRY_NAME='United States', count=211),\n",
       " Row(DEST_COUNTRY_NAME='Latvia', ORIGIN_COUNTRY_NAME='United States', count=19),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ecuador', count=300),\n",
       " Row(DEST_COUNTRY_NAME='Suriname', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Mexico', ORIGIN_COUNTRY_NAME='United States', count=7140),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cyprus', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Ecuador', ORIGIN_COUNTRY_NAME='United States', count=268),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Portugal', count=134),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Costa Rica', count=608),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guatemala', count=318),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Suriname', count=34),\n",
       " Row(DEST_COUNTRY_NAME='Colombia', ORIGIN_COUNTRY_NAME='United States', count=873),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cape Verde', count=14),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Jamaica', count=712),\n",
       " Row(DEST_COUNTRY_NAME='Norway', ORIGIN_COUNTRY_NAME='United States', count=121),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malaysia', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Morocco', count=19),\n",
       " Row(DEST_COUNTRY_NAME='Thailand', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Samoa', count=25),\n",
       " Row(DEST_COUNTRY_NAME='Venezuela', ORIGIN_COUNTRY_NAME='United States', count=290),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Palau', count=31),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Venezuela', count=246),\n",
       " Row(DEST_COUNTRY_NAME='Panama', ORIGIN_COUNTRY_NAME='United States', count=510),\n",
       " Row(DEST_COUNTRY_NAME='Antigua and Barbuda', ORIGIN_COUNTRY_NAME='United States', count=126),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Chile', count=185),\n",
       " Row(DEST_COUNTRY_NAME='Morocco', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Finland', count=28),\n",
       " Row(DEST_COUNTRY_NAME='Azerbaijan', ORIGIN_COUNTRY_NAME='United States', count=21),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greece', count=23),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='The Bahamas', count=986),\n",
       " Row(DEST_COUNTRY_NAME='New Zealand', ORIGIN_COUNTRY_NAME='United States', count=111),\n",
       " Row(DEST_COUNTRY_NAME='Liberia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hong Kong', count=414),\n",
       " Row(DEST_COUNTRY_NAME='Hungary', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='China', count=920),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Vietnam', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Burkina Faso', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Sweden', ORIGIN_COUNTRY_NAME='United States', count=118),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kuwait', count=28),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominican Republic', count=1420),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Egypt', count=12),\n",
       " Row(DEST_COUNTRY_NAME='Israel', ORIGIN_COUNTRY_NAME='United States', count=134),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=370002),\n",
       " Row(DEST_COUNTRY_NAME='Ethiopia', ORIGIN_COUNTRY_NAME='United States', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Luxembourg', count=134),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Poland', count=33),\n",
       " Row(DEST_COUNTRY_NAME='Martinique', ORIGIN_COUNTRY_NAME='United States', count=44),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Barthelemy', count=41),\n",
       " Row(DEST_COUNTRY_NAME='Saint Barthelemy', ORIGIN_COUNTRY_NAME='United States', count=39),\n",
       " Row(DEST_COUNTRY_NAME='Barbados', ORIGIN_COUNTRY_NAME='United States', count=154),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Turkey', count=129),\n",
       " Row(DEST_COUNTRY_NAME='Djibouti', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Azerbaijan', count=21),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Estonia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Germany', ORIGIN_COUNTRY_NAME='United States', count=1468),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='South Korea', count=827),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='El Salvador', count=508),\n",
       " Row(DEST_COUNTRY_NAME='Ireland', ORIGIN_COUNTRY_NAME='United States', count=335),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Hungary', count=3),\n",
       " Row(DEST_COUNTRY_NAME='Zambia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Malaysia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ethiopia', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Panama', count=465),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Aruba', count=342),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Thailand', count=4),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Turks and Caicos Islands', count=236),\n",
       " Row(DEST_COUNTRY_NAME='Croatia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Pakistan', count=12),\n",
       " Row(DEST_COUNTRY_NAME='Cyprus', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Honduras', count=407),\n",
       " Row(DEST_COUNTRY_NAME='Fiji', ORIGIN_COUNTRY_NAME='United States', count=24),\n",
       " Row(DEST_COUNTRY_NAME='Qatar', ORIGIN_COUNTRY_NAME='United States', count=108),\n",
       " Row(DEST_COUNTRY_NAME='Saint Kitts and Nevis', ORIGIN_COUNTRY_NAME='United States', count=139),\n",
       " Row(DEST_COUNTRY_NAME='Kuwait', ORIGIN_COUNTRY_NAME='United States', count=32),\n",
       " Row(DEST_COUNTRY_NAME='Taiwan', ORIGIN_COUNTRY_NAME='United States', count=266),\n",
       " Row(DEST_COUNTRY_NAME='Haiti', ORIGIN_COUNTRY_NAME='United States', count=226),\n",
       " Row(DEST_COUNTRY_NAME='Canada', ORIGIN_COUNTRY_NAME='United States', count=8399),\n",
       " Row(DEST_COUNTRY_NAME='Federated States of Micronesia', ORIGIN_COUNTRY_NAME='United States', count=69),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Liberia', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Jamaica', ORIGIN_COUNTRY_NAME='United States', count=666),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Malta', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Dominican Republic', ORIGIN_COUNTRY_NAME='United States', count=1353),\n",
       " Row(DEST_COUNTRY_NAME='Japan', ORIGIN_COUNTRY_NAME='United States', count=1548),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Lithuania', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Finland', ORIGIN_COUNTRY_NAME='United States', count=26),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guadeloupe', count=59),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ukraine', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='France', count=952),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Norway', count=115),\n",
       " Row(DEST_COUNTRY_NAME='Aruba', ORIGIN_COUNTRY_NAME='United States', count=346),\n",
       " Row(DEST_COUNTRY_NAME='French Guiana', ORIGIN_COUNTRY_NAME='United States', count=5),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Kiribati', count=35),\n",
       " Row(DEST_COUNTRY_NAME='India', ORIGIN_COUNTRY_NAME='United States', count=61),\n",
       " Row(DEST_COUNTRY_NAME='British Virgin Islands', ORIGIN_COUNTRY_NAME='United States', count=107),\n",
       " Row(DEST_COUNTRY_NAME='Brazil', ORIGIN_COUNTRY_NAME='United States', count=853),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Germany', count=1336),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='New Zealand', count=74),\n",
       " Row(DEST_COUNTRY_NAME='French Polynesia', ORIGIN_COUNTRY_NAME='United States', count=43),\n",
       " Row(DEST_COUNTRY_NAME='United Arab Emirates', ORIGIN_COUNTRY_NAME='United States', count=320),\n",
       " Row(DEST_COUNTRY_NAME='Singapore', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Mexico', count=7187),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Sweden', count=119),\n",
       " Row(DEST_COUNTRY_NAME='Netherlands', ORIGIN_COUNTRY_NAME='United States', count=776),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Martinique', count=43),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United Arab Emirates', count=313),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bulgaria', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Denmark', ORIGIN_COUNTRY_NAME='United States', count=153),\n",
       " Row(DEST_COUNTRY_NAME='China', ORIGIN_COUNTRY_NAME='United States', count=772),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Nicaragua', count=201),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Philippines', count=126),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Georgia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Belgium', count=228),\n",
       " Row(DEST_COUNTRY_NAME='Cayman Islands', ORIGIN_COUNTRY_NAME='United States', count=314),\n",
       " Row(DEST_COUNTRY_NAME='Argentina', ORIGIN_COUNTRY_NAME='United States', count=180),\n",
       " Row(DEST_COUNTRY_NAME='Peru', ORIGIN_COUNTRY_NAME='United States', count=279),\n",
       " Row(DEST_COUNTRY_NAME='South Africa', ORIGIN_COUNTRY_NAME='United States', count=36),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Iceland', count=202),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Argentina', count=141),\n",
       " Row(DEST_COUNTRY_NAME='Spain', ORIGIN_COUNTRY_NAME='United States', count=420),\n",
       " Row(DEST_COUNTRY_NAME='Bermuda', ORIGIN_COUNTRY_NAME='United States', count=183),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Nigeria', count=50),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Austria', count=63),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', count=59),\n",
       " Row(DEST_COUNTRY_NAME='Kiribati', ORIGIN_COUNTRY_NAME='United States', count=26),\n",
       " Row(DEST_COUNTRY_NAME='Saudi Arabia', ORIGIN_COUNTRY_NAME='United States', count=83),\n",
       " Row(DEST_COUNTRY_NAME='Czech Republic', ORIGIN_COUNTRY_NAME='United States', count=13),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Israel', count=127),\n",
       " Row(DEST_COUNTRY_NAME='Belgium', ORIGIN_COUNTRY_NAME='United States', count=259),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Lucia', count=136),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bahrain', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='British Virgin Islands', count=80),\n",
       " Row(DEST_COUNTRY_NAME='Curacao', ORIGIN_COUNTRY_NAME='United States', count=90),\n",
       " Row(DEST_COUNTRY_NAME='Georgia', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Denmark', count=152),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Guyana', count=63),\n",
       " Row(DEST_COUNTRY_NAME='Philippines', ORIGIN_COUNTRY_NAME='United States', count=134),\n",
       " Row(DEST_COUNTRY_NAME='Grenada', ORIGIN_COUNTRY_NAME='United States', count=53),\n",
       " Row(DEST_COUNTRY_NAME='Cape Verde', ORIGIN_COUNTRY_NAME='United States', count=20),\n",
       " Row(DEST_COUNTRY_NAME=\"Cote d'Ivoire\", ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Ukraine', ORIGIN_COUNTRY_NAME='United States', count=14),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Papua New Guinea', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Russia', ORIGIN_COUNTRY_NAME='United States', count=176),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saudi Arabia', count=70),\n",
       " Row(DEST_COUNTRY_NAME='Guatemala', ORIGIN_COUNTRY_NAME='United States', count=397),\n",
       " Row(DEST_COUNTRY_NAME='Saint Lucia', ORIGIN_COUNTRY_NAME='United States', count=123),\n",
       " Row(DEST_COUNTRY_NAME='Paraguay', ORIGIN_COUNTRY_NAME='United States', count=60),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Curacao', count=83),\n",
       " Row(DEST_COUNTRY_NAME='Kosovo', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Taiwan', count=235),\n",
       " Row(DEST_COUNTRY_NAME='Tunisia', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='South Africa', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Niger', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='Turkey', ORIGIN_COUNTRY_NAME='United States', count=138),\n",
       " Row(DEST_COUNTRY_NAME='United Kingdom', ORIGIN_COUNTRY_NAME='United States', count=2025),\n",
       " Row(DEST_COUNTRY_NAME='Romania', ORIGIN_COUNTRY_NAME='United States', count=14),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Greenland', count=4),\n",
       " Row(DEST_COUNTRY_NAME='Papua New Guinea', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Spain', count=442),\n",
       " Row(DEST_COUNTRY_NAME='Iraq', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Italy', count=438),\n",
       " Row(DEST_COUNTRY_NAME='Cuba', ORIGIN_COUNTRY_NAME='United States', count=466),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Switzerland', count=305),\n",
       " Row(DEST_COUNTRY_NAME='Dominica', ORIGIN_COUNTRY_NAME='United States', count=20),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Japan', count=1496),\n",
       " Row(DEST_COUNTRY_NAME='Portugal', ORIGIN_COUNTRY_NAME='United States', count=127),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Brazil', count=619),\n",
       " Row(DEST_COUNTRY_NAME='Bahrain', ORIGIN_COUNTRY_NAME='United States', count=19),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Peru', count=337),\n",
       " Row(DEST_COUNTRY_NAME='Indonesia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Belize', count=193),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United Kingdom', count=1970),\n",
       " Row(DEST_COUNTRY_NAME='Belize', ORIGIN_COUNTRY_NAME='United States', count=188),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ghana', count=20),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Indonesia', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Fiji', count=25),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Canada', count=8483),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Antigua and Barbuda', count=117),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='French Polynesia', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Nicaragua', ORIGIN_COUNTRY_NAME='United States', count=179),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Latvia', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Dominica', count=27),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Czech Republic', count=12),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Australia', count=258),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cook Islands', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Austria', ORIGIN_COUNTRY_NAME='United States', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Jordan', ORIGIN_COUNTRY_NAME='United States', count=44),\n",
       " Row(DEST_COUNTRY_NAME='Palau', ORIGIN_COUNTRY_NAME='United States', count=30),\n",
       " Row(DEST_COUNTRY_NAME='South Korea', ORIGIN_COUNTRY_NAME='United States', count=1048),\n",
       " Row(DEST_COUNTRY_NAME='Angola', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='Ghana', ORIGIN_COUNTRY_NAME='United States', count=18),\n",
       " Row(DEST_COUNTRY_NAME='New Caledonia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Guadeloupe', ORIGIN_COUNTRY_NAME='United States', count=56),\n",
       " Row(DEST_COUNTRY_NAME='France', ORIGIN_COUNTRY_NAME='United States', count=935),\n",
       " Row(DEST_COUNTRY_NAME='Poland', ORIGIN_COUNTRY_NAME='United States', count=32),\n",
       " Row(DEST_COUNTRY_NAME='Nigeria', ORIGIN_COUNTRY_NAME='United States', count=59),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Uruguay', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Greenland', ORIGIN_COUNTRY_NAME='United States', count=2),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bermuda', count=193),\n",
       " Row(DEST_COUNTRY_NAME='Chile', ORIGIN_COUNTRY_NAME='United States', count=174),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cuba', count=478),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Montenegro', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Colombia', count=867),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Barbados', count=130),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Qatar', count=109),\n",
       " Row(DEST_COUNTRY_NAME='Australia', ORIGIN_COUNTRY_NAME='United States', count=329),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cayman Islands', count=310),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Jordan', count=44),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Namibia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Trinidad and Tobago', count=217),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bolivia', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Cook Islands', ORIGIN_COUNTRY_NAME='United States', count=13),\n",
       " Row(DEST_COUNTRY_NAME='Bulgaria', ORIGIN_COUNTRY_NAME='United States', count=3),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Saint Kitts and Nevis', count=145),\n",
       " Row(DEST_COUNTRY_NAME='Uruguay', ORIGIN_COUNTRY_NAME='United States', count=43),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Haiti', count=225),\n",
       " Row(DEST_COUNTRY_NAME='Bonaire, Sint Eustatius, and Saba', ORIGIN_COUNTRY_NAME='United States', count=58),\n",
       " Row(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable and print the result with show,take,show(value,False),collect\n",
    "json_data.show(5)\n",
    "json_data.take(5)\n",
    "json_data.show(2,False)\n",
    "json_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "666b9d35-2abe-46c2-861e-33a44cea73fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#create a temp table retail_temp_table and print schema of retail_data\n",
    "retail_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.show(5)\n",
    "retail_data.createOrReplaceTempView(\"retail_temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36d7fe92-a53c-40f7-b8a2-18f70c185f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, 0.5: double, five: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lit and select the 5,0.5,five\n",
    "from pyspark.sql.functions import lit\n",
    "retail_data.select(lit(5),lit(0.5),lit(\"five\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3761559a-52f2-473c-ad1d-cd97795c22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|invoiceno|         description|\n",
      "+---------+--------------------+\n",
      "|   536366|HAND WARMER UNION...|\n",
      "|   536366|HAND WARMER RED P...|\n",
      "|   536367|ASSORTED COLOUR B...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "|   536367|POPPY'S PLAYHOUSE...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from retail_data select invoiceno,description where InvoiceNo not equal to 536365 and show the result\n",
    "from pyspark.sql.functions import col\n",
    "a=retail_data.select(col(\"invoiceno\"),col(\"description\")).where(col(\"InvoiceNo\")!=536365)\n",
    "a.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2c6bcfd-b0ca-4990-8446-3358cd3b0124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#in pricefilter variable check UnitPrice is grater tan 600\n",
    "#in descriptionfilter variable check Description col contains POSTAGE and greater than or equal to 1 with instr\n",
    "#with reatial_data check StockCode is dot with == and pricefilter or descriptionfilter and show\n",
    "#with reatial_data check StockCode is dot with isin and pricefilter or descriptionfilter and show\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "retail_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"2010-12-01.csv\")\n",
    "pricefilter=col(\"UnitPrice\")>600\n",
    "descriptionfilter=instr(col(\"Description\"),\"POSTAGE\") >=1\n",
    "retail_data.where(col(\"StockCode\")==\"DOT\").where(pricefilter | descriptionfilter).show(5)\n",
    "\n",
    "\n",
    "retail_data.where((col(\"StockCode\")==\"DOT\") & (pricefilter | descriptionfilter)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "987bd895-5e56-4460-860e-2918888f7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isexpensive|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|       true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|       true|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#in pricefilter variable check UnitPrice is grater tan 600\n",
    "#in stockcodefilter variable check stockcodefilter contains dot init\n",
    "#in descriptionfilter variable check Description col contains POSTAGE and greater than or equal to 1 with instr\n",
    "#with reatial_data add new column isexpensive where  descriptionfilter and (pricefilter or stockcodefilter) in checking_filters variable\n",
    "#and show if isexpensive is true\n",
    "\n",
    "pricefilter=col(\"UnitPrice\") > 600\n",
    "stockcodefilter=col(\"StockCode\").isin(\"DOT\")\n",
    "descriptionfilter=instr(col(\"Description\"),\"POSTAGE\")>=1\n",
    "retail_data.withColumn(\"isexpensive\",descriptionfilter & (pricefilter | stockcodefilter)).where(\"isexpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4bd9cd08-2c54-4adc-b22d-0820566c476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isexpensive|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|       true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|       true|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#add new column isexpensive where not(unitprice) is less than or equal to 250 and show if isexpensive is true\n",
    "from pyspark.sql.functions import expr\n",
    "retail_data.withColumn(\"isexpensive\",expr(\"not unitprice<=250\")).where(\"isexpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e056ff9c-8bd5-444e-b1a4-475c00501a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+---+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|new|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+---+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=retail_data.where(col(\"Description\").eqNullSafe(\"hello\"))\n",
    "a.withColumn(\"new\",col(\"Description\").isin(\"hello\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2befc3a3-7a1d-492e-9166-dd97c70f9c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|            actual|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|CustomerID|            actual|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#using pow function ((Quantity*UnitPrice) ^2 )+5 store in fabricated_quantity variable\n",
    "#and show the result with just select statement of 2 records of columns CustomerID and its fabricated_quantity as \"actual\"\n",
    "#and show the result with just selectexpr statement of 2 records of columns CustomerID and its fabricated_quantity as \"actual\"\n",
    "from pyspark.sql.functions import pow\n",
    "fabricated_quantity=pow((col(\"Quantity\")*col(\"UnitPrice\")),2)+5\n",
    "retail_data.select(col(\"CustomerID\"),fabricated_quantity.alias(\"actual\")).show(2)\n",
    "\n",
    "retail_data.selectExpr(\"CustomerID\",\"POWER(Quantity*UnitPrice,2)+5 as actual\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "902255e8-e625-4630-aefd-fd8bf8c09bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#with round(lit(2.5)),bround(lit(2.5)) check what does the output\n",
    "from pyspark.sql.functions import lit,round,bround\n",
    "retail_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.select(round(lit(2.5)),bround(lit(2.5))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c84c874-4897-4045-a4ac-739a04503ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(UnitPrice, Quantity)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835552|\n",
      "+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.04112314436835552"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#with corr between columns UnitPrice,Quantity check how much its linear relationship \n",
    "#with describe function check the retail_data of count,min,max,mean,stddev\n",
    "from pyspark.sql.functions import corr\n",
    "retail_data.select(corr(\"UnitPrice\",\"Quantity\")).show(1)\n",
    "retail_data.stat.corr(\"UnitPrice\",\"Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a001a95b-d5d1-4b18-9fe1-a6020c83e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                NULL| 8.627413127413128| 4.151946589446603|15661.388719512195|          NULL|\n",
      "| stddev|72.89447869788873|17407.897548583845|                NULL|26.371821677029203|15.638659854603892|1854.4496996893627|          NULL|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06052fad-2cea-416c-9121-59bd47052ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|            84029E|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  5|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|\n",
      "|             22386|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[22086, 21705, 72...|[200, 128, 23, 50...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+-----------------------------+---------+\n",
      "|monotonically_increasing_id()|StockCode|\n",
      "+-----------------------------+---------+\n",
      "|                            0|   85123A|\n",
      "|                            1|    71053|\n",
      "+-----------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#colname variable contains unitprice,quantileprobs variable contains list of 0.5 value, relerror contains 0.05 value\n",
    "#check .stat.approxQuantile with 3 varibales with retail_data\n",
    "#check .stat.crosstab with \"StockCode\",\"Quantity\" varibales with retail_data\n",
    "#check .stat.freqItems with \"StockCode\",\"Quantity\" varibales with retail_data\n",
    "#with monotonically_increasing_id select and show 2 records\n",
    "from pyspark.sql.functions import col,monotonically_increasing_id\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "colname=\"UnitPrice\"\n",
    "quantileprobs=[0.5]\n",
    "relerror=0.05\n",
    "retail_data.stat.approxQuantile(colname,quantileprobs,relerror)\n",
    "retail_data.stat.crosstab(\"StockCode\",\"Quantity\").show(2)\n",
    "retail_data.stat.freqItems([\"StockCode\",\"Quantity\"]).show(2)\n",
    "retail_data.select(monotonically_increasing_id(),col(\"StockCode\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e32baae1-9390-4806-87fe-8ef9ab1679c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |initcap(Description)              |upper(Description)                |lower(Description)                |\n",
      "+----------------------------------+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|White Hanging Heart T-light Holder|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|\n",
      "|WHITE METAL LANTERN               |White Metal Lantern               |WHITE METAL LANTERN               |white metal lantern               |\n",
      "+----------------------------------+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#use function initcap,upper,lower for description variable and show result\n",
    "from pyspark.sql.functions import initcap,upper,lower\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.select(\"Description\",initcap(\"Description\"),upper(\"Description\"),lower(\"Description\")).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "341e2ab7-e5ae-45c5-b903-a986bc0dbe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|    HELLO    |    ltrim|\n",
      "+-------------+---------+\n",
      "|    HELLO    |HELLO    |\n",
      "+-------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+---------+\n",
      "|    HELLO    |    ltrim|\n",
      "+-------------+---------+\n",
      "|    HELLO    |    HELLO|\n",
      "+-------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+--------------+\n",
      "|    HELLO    |          rpad|\n",
      "+-------------+--------------+\n",
      "|    HELLO    |    HELLO    x|\n",
      "+-------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+--------------+\n",
      "|    HELLO    |          lpad|\n",
      "+-------------+--------------+\n",
      "|    HELLO    |x    HELLO    |\n",
      "+-------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+-----+\n",
      "|    HELLO    | lpad|\n",
      "+-------------+-----+\n",
      "|    HELLO    |    H|\n",
      "+-------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using ltrim,rtrim,rpad,lpad,trim select (\"    HELLO    \") and check the result\n",
    "from pyspark.sql.functions import ltrim,rtrim,rpad,lpad,lit\n",
    "retail_data.select(lit(\"    HELLO    \"),ltrim(lit(\"    HELLO    \")).alias(\"ltrim\")).show(1)\n",
    "retail_data.select(lit(\"    HELLO    \"),rtrim(lit(\"    HELLO    \")).alias(\"ltrim\")).show(1)\n",
    "retail_data.select(lit(\"    HELLO    \"),rpad(lit(\"    HELLO    \"),14,\"x\").alias(\"rpad\")).show(1)\n",
    "retail_data.select(lit(\"    HELLO    \"),lpad(lit(\"    HELLO    \"),14,\"x\").alias(\"lpad\")).show(1)\n",
    "retail_data.select(lit(\"    HELLO    \"),lpad(lit(\"    HELLO    \"),5,\"x\").alias(\"lpad\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f329fb3-8fae-4667-b7a1-dcb8e6d551a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------------------------------------+\n",
      "|         Description|regexp_replace(Description, BLACK|WHITE|RED|GREEN|BLUE, sush, 1)|\n",
      "+--------------------+----------------------------------------------------------------+\n",
      "|WHITE HANGING HEA...|                                            sush HANGING HEAR...|\n",
      "| WHITE METAL LANTERN|                                              sush METAL LANTERN|\n",
      "+--------------------+----------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+-------------------------------------+\n",
      "|         Description|translate(Description, HANGING, 1234)|\n",
      "+--------------------+-------------------------------------+\n",
      "|WHITE HANGING HEA...|                 W1TE 123434 1E2RT...|\n",
      "| WHITE METAL LANTERN|                   W1TE MET2L L23TER3|\n",
      "+--------------------+-------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using regexp_replace wherever col description contains BLACK|WHITE|RED|GREEN|BLUE then replaced with sush\n",
    "#using translate wherever hanging comes replace with 1234\n",
    "from pyspark.sql.functions import regexp_replace,regexp_extract,translate\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "string_val=\"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "retail_data.select(\"Description\",regexp_replace(\"Description\",string_val,\"sush\")).show(2)\n",
    "retail_data.select(\"Description\",translate(\"Description\",\"HANGING\",\"1234\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a9d62f3-9fc4-48e8-85fe-f157d038b44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------------------------+\n",
      "|Description                        |regexp_extract(Description, (BLACK|WHITE|RED|GREEN|BLUE), 0)|\n",
      "+-----------------------------------+------------------------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |WHITE                                                       |\n",
      "|WHITE METAL LANTERN                |WHITE                                                       |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |                                                            |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|                                                            |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |RED                                                         |\n",
      "+-----------------------------------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using regexp_extract wherever col description contains BLACK|WHITE|RED|GREEN|BLUE then extract the first\n",
    "# For each row, it returns the extracted color (if any) along with the original \"Description\" value and then displays the first 5 result\n",
    "retail_data.select(\"Description\",regexp_extract(\"Description\",\"(BLACK|WHITE|RED|GREEN|BLUE)\",0)).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e3eaef6-15f4-47ea-b688-89a54df95741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------------+\n",
      "|haswhiteorblack|Description                        |\n",
      "+---------------+-----------------------------------+\n",
      "|true           |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|true           |WHITE METAL LANTERN                |\n",
      "|false          |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|false          |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|true           |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#contains_black variable check col(Description) contains black,contains_white variable check col(Description) contains white\n",
    "#add new column to retail_data haswhiteorblack with or condition check it contains black or white and that is true and select only description\n",
    "from pyspark.sql.functions import instr\n",
    "contains_black=instr(col(\"Description\"),\"black\") >=1\n",
    "contains_white=instr(col(\"Description\"),\"WHITE\") >=1\n",
    "retail_data.withColumn(\"haswhiteorblack\",contains_black | contains_white).select(\"haswhiteorblack\",\"Description\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec2db791-4457-434c-ae31-70f4e1f9542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS is_black'>, Column<'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS is_white'>, Column<'CAST(locate(RED, Description, 1) AS BOOLEAN) AS is_red'>, Column<'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS is_blue'>, Column<'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS is_green'>]\n",
      "+--------+--------+------+-------+--------+----------------------------------+\n",
      "|is_black|is_white|is_red|is_blue|is_green|Description                       |\n",
      "+--------+--------+------+-------+--------+----------------------------------+\n",
      "|false   |true    |false |false  |false   |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|false   |true    |false |false  |false   |WHITE METAL LANTERN               |\n",
      "|false   |true    |true  |false  |false   |RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+--------+--------+------+-------+--------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a list simplecolors=[\"black\",\"white\",\"red\",\"blue\",\"green\"]\n",
    "#selectedcolumns varibale is an empty list\n",
    "#create a function colo_locator \n",
    "#you have a DataFrame named retail_data with a column Description containing text descriptions of retail items. \n",
    "#You want to create new boolean columns indicating the presence of specific colors in the Description column with is_red,is_black etc\n",
    "#After that, you wish to filter and display the rows where the Description contains either \"white\" or \"red\".\n",
    "from pyspark.sql.functions import locate,col,expr\n",
    "simplecolors=[\"black\",\"white\",\"red\",\"blue\",\"green\"]\n",
    "selectedcolumns=[]\n",
    "\n",
    "def color_locator(column,color_string):\n",
    "    return locate(color_string.upper(),column).cast(\"boolean\").alias(\"is_\"+color_string)\n",
    "for i in simplecolors:\n",
    "    a=color_locator(retail_data.Description,i)\n",
    "    selectedcolumns.append(a)\n",
    "print(selectedcolumns)\n",
    "selectedcolumns.append(col(\"Description\"))\n",
    "retail_data.select(*selectedcolumns).where(expr(\"is_white or is_red\")).show(3,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d184247-1ae4-4fbc-a18e-d48d1788b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------------+\n",
      "|id |today     |now                       |\n",
      "+---+----------+--------------------------+\n",
      "|0  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|1  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|2  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|3  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|4  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|5  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|6  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|7  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|8  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "|9  |2025-05-19|2025-05-19 16:50:43.880564|\n",
      "+---+----------+--------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe datadf of range(10) and adding two columns today column contains current_date and now column contains current_timestamp\n",
    "#create a temp table \"datatable\" with datadf\n",
    "#print the schema of datadf\n",
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "datadf=spark.range(10).withColumn(\"today\",current_date()).withColumn(\"now\",current_timestamp())\n",
    "datadf.createOrReplaceTempView(\"datatable\")\n",
    "datadf.schema\n",
    "datadf.show(11,False)\n",
    "datadf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "670355f4-069d-4aa6-b104-4f72a71fc7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_add(today, 5)|date_sub(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2025-05-24|        2025-05-14|\n",
      "|        2025-05-24|        2025-05-14|\n",
      "+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import date_add,date_sub and with existing datadf dataframe ad 5 days to today column and subtract 5 days to today column and show\n",
    "from pyspark.sql.functions import date_add,date_sub\n",
    "datadf.select(date_add(\"today\",5),date_sub(\"today\",5)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f365d983-03a8-4f2c-8189-f5c509d7cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(today, week_age)|\n",
      "+-------------------------+\n",
      "|                        7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------------------------------+\n",
      "|months_between(start_date, end_date, true)|\n",
      "+------------------------------------------+\n",
      "|                              -16.67741935|\n",
      "|                              -16.67741935|\n",
      "+------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import 3 functions datediff,months_between,to_date\n",
    "#with existing datadf dataframe add new column week_age contains data of subtract 7 days of today column and checkdiff with datediff and show diff\n",
    "#with existing datadf dataframe lit(\"2016-01-01\"),lit(\"2017-05-22\") convert the 2 lit into date with to_date and start_date and end_date columns\n",
    "#select months_between diff of 2 columns of start_date and end_date columns\n",
    "from pyspark.sql.functions import datediff,months_between,to_date,lit\n",
    "datadf.withColumn(\"week_age\",date_sub(\"today\",7)).select(datediff(\"today\",\"week_age\")).show(1)\n",
    "datadf.withColumn(\"start_date\",to_date(lit(\"2016-01-01\"))).withColumn(\"end_date\",to_date(lit(\"2017-05-22\"))) \\\n",
    "        .select(months_between(\"start_date\",\"end_date\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "efab927b-e865-4a9d-b758-97f7cccc88f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2000-10-16)|to_date(2000-16-10)|\n",
      "+-------------------+-------------------+\n",
      "|         2000-10-16|               NULL|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe with range(5) and add new column \"date\" contains value of \"2000-10-16\" and while select convert into str to date with to_date\n",
    "#with existing datadf dataframe select \"2000-10-16\",\"2000-16-10\" by converting into date with to_date\n",
    "\n",
    "df=spark.range(5).withColumn(\"date\",lit(\"2000-10-16\")).select(to_date(\"date\"))\n",
    "df.select(to_date(lit(\"2000-10-16\")),to_date(lit(\"2000-16-10\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ce53042-02d9-4adf-b2d7-6b8e8958d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     first|    second|\n",
      "+----------+----------+\n",
      "|2000-10-16|2000-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+-------------------+--------------------+\n",
      "|to_timestamp(first)|to_timestamp(second)|\n",
      "+-------------------+--------------------+\n",
      "|2000-10-16 00:00:00| 2000-12-20 00:00:00|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "+-----+------+\n",
      "|first|second|\n",
      "+-----+------+\n",
      "+-----+------+\n",
      "\n",
      "+----------+----------+\n",
      "|     first|    second|\n",
      "+----------+----------+\n",
      "|2000-10-16|2000-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import to_timestamp function \n",
    "#in dateFormat variable store the format 'yyyy-dd-MM'\n",
    "#create dataframe range(1) and\n",
    "#selectint 2 columns with to_date 1 is \"first\" contains value \"2000-16-10\" and \"second\" contains value \"2000-20-12\"\n",
    "#with to_timestamp select first and second columns\n",
    "from pyspark.sql.functions import to_timestamp,col\n",
    "dateFormat=\"yyyy-dd-MM\"\n",
    "df=spark.range(1).select(to_date(lit(\"2000-16-10\"),dateFormat).alias(\"first\"),to_date(lit(\"2000-20-12\"),dateFormat).alias(\"second\"))\n",
    "df.show(1)\n",
    "df.select(to_timestamp(\"first\"),to_timestamp(\"second\")).show(1)\n",
    "df.filter(col(\"first\")>col(\"second\")).show(1)\n",
    "df.filter(col(\"first\")<\"2000-10-17\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af79d42b-e1ea-459b-bf97-48faa5913458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+\n",
      "|coalesce(CustomerID)|\n",
      "+--------------------+\n",
      "|17850.0             |\n",
      "|17850.0             |\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in df variable read the 2010-12-01.csv data and check the CustomerID column contains null or not\n",
    "from pyspark.sql.functions import coalesce\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"2010-12-01.csv\")\n",
    "df.show(2)\n",
    "df.select(coalesce(col(\"CustomerID\"))).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eaa4c2dc-2c49-487e-b2f8-37fdf62b655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|  name|roll_no|class|\n",
      "+------+-------+-----+\n",
      "| first|      1|  1st|\n",
      "|second|      1|  2nd|\n",
      "| third|      2|  1st|\n",
      "|fourth|      2|  2nd|\n",
      "|  NULL|   NULL| NULL|\n",
      "|  sush|   NULL|  1st|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+-----+\n",
      "|  name|roll_no|class|\n",
      "+------+-------+-----+\n",
      "| first|      1|  1st|\n",
      "|second|      1|  2nd|\n",
      "| third|      2|  1st|\n",
      "|fourth|      2|  2nd|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+-----+\n",
      "|  name|roll_no|class|\n",
      "+------+-------+-----+\n",
      "| first|      1|  1st|\n",
      "|second|      1|  2nd|\n",
      "| third|      2|  1st|\n",
      "|fourth|      2|  2nd|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+-----+\n",
      "|  name|roll_no|class|\n",
      "+------+-------+-----+\n",
      "| first|      1|  1st|\n",
      "|second|      1|  2nd|\n",
      "| third|      2|  1st|\n",
      "|fourth|      2|  2nd|\n",
      "|  sush|   NULL|  1st|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+-----+\n",
      "|  name|roll_no|class|\n",
      "+------+-------+-----+\n",
      "| first|      1|  1st|\n",
      "|second|      1|  2nd|\n",
      "| third|      2|  1st|\n",
      "|fourth|      2|  2nd|\n",
      "|  sush|   NULL|  1st|\n",
      "+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe in df variable like below\n",
    "# +------+-------+-----+\n",
    "# |  name|roll_no|class|\n",
    "# +------+-------+-----+\n",
    "# | first|      1|  1st|\n",
    "# |second|      1|  2nd|\n",
    "# | third|      2|  1st|\n",
    "# |fourth|      2|  2nd|\n",
    "# |  NULL|   NULL| NULL|\n",
    "# |  sush|   NULL|  1st|\n",
    "# +------+-------+-----+\n",
    "#drop the records with any,all,all with specific columns \n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType\n",
    "from pyspark.sql import Row\n",
    "manual_schema=StructType([StructField(\"name\",StringType(),True),StructField(\"roll_no\",LongType(),True),StructField(\"class\",StringType(),True)])\n",
    "new_rows=[Row(\"first\",1,\"1st\"),Row(\"second\",1,\"2nd\"),Row(\"third\",2,\"1st\"),Row(\"fourth\",2,\"2nd\"),Row(None,None,None),Row(\"sush\",None,\"1st\")]\n",
    "new_df=spark.sparkContext.parallelize(new_rows)\n",
    "df=spark.createDataFrame(new_df,manual_schema)\n",
    "df.show()\n",
    "df.na.drop().show()\n",
    "df.na.drop(\"any\").show()\n",
    "df.na.drop(\"all\").show()\n",
    "df.na.drop(\"all\",subset=[\"name\",\"roll_no\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8f447e9-5859-40bd-9cac-d3e2ac8f4f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------------+\n",
      "|         name|roll_no|        class|\n",
      "+-------------+-------+-------------+\n",
      "|        first|      1|          1st|\n",
      "|       second|      1|          2nd|\n",
      "|        third|      2|          1st|\n",
      "|       fourth|      2|          2nd|\n",
      "|nulling_value|   NULL|nulling_value|\n",
      "|         sush|   NULL|          1st|\n",
      "+-------------+-------+-------------+\n",
      "\n",
      "+-------+-------+-------+\n",
      "|   name|roll_no|  class|\n",
      "+-------+-------+-------+\n",
      "|  first|      1|    1st|\n",
      "| second|      1|    2nd|\n",
      "|  third|      2|    1st|\n",
      "| fourth|      2|    2nd|\n",
      "|nulling|   NULL|nulling|\n",
      "|   sush|   NULL|    1st|\n",
      "+-------+-------+-------+\n",
      "\n",
      "+---------+-------+-----+\n",
      "|     name|roll_no|class|\n",
      "+---------+-------+-----+\n",
      "|    first|      1|  1st|\n",
      "|   second|      1|  2nd|\n",
      "|    third|      2|  1st|\n",
      "|   fourth|      2|  2nd|\n",
      "|new_value|      5| temp|\n",
      "|     sush|      5|  1st|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with the same df now fill the values in 2 different ways\n",
    "#first is filling values with nulling for specific columns\n",
    "#second is filling values with each column each different value\n",
    "df.na.fill(\"nulling_value\").show()\n",
    "df.na.fill(\"nulling\",subset=[\"name\",\"roll_no\",\"class\"]).show()\n",
    "filling={\"name\":\"new_value\",\"roll_no\":5,\"class\":\"temp\"}\n",
    "df.na.fill(filling).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1363b0f0-ccad-43c5-935e-f247609352b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|  name|roll_no|class|\n",
      "+------+-------+-----+\n",
      "| first|      1|  1st|\n",
      "|second|      1|  2nd|\n",
      "| third|      2|  1st|\n",
      "|fourth|      2|  2nd|\n",
      "|  NULL|   NULL| NULL|\n",
      "|  sush|   NULL|  1st|\n",
      "+------+-------+-----+\n",
      "\n",
      "+------+-------+-------+\n",
      "|  name|roll_no|  class|\n",
      "+------+-------+-------+\n",
      "| first|      1|unknown|\n",
      "|second|      1|    2nd|\n",
      "| third|      2|unknown|\n",
      "|fourth|      2|    2nd|\n",
      "|  NULL|   NULL|   NULL|\n",
      "|  sush|   NULL|unknown|\n",
      "+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using replace the column values to a different value\n",
    "df.show()\n",
    "df.na.replace([\"1st\"],[\"unknown\"],\"class\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89c33177-e963-4b4f-8873-1cd34c99158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+--------------------------------------------+\n",
      "|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |complex                                     |\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+--------------------------------------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|\n",
      "|536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|{WHITE METAL LANTERN, 536365}               |\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+--------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import struct, using struct select \"Description\",\"InvoiceNo\" columns as \"complex\"\n",
    "#create a table complexDF_table from the complexDF dataframe\n",
    "from pyspark.sql.functions import struct\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "complexDF=df.select(\"*\",struct(\"Description\",\"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.show(2,False)\n",
    "complexDF.createOrReplaceTempView(\"complexDF_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c961fd05-648f-4d37-aeb8-80d8b115bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+----------------------------------+-----------------+\n",
      "|complex                                     |complex.Description               |complex.InvoiceNo|\n",
      "+--------------------------------------------+----------------------------------+-----------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|WHITE HANGING HEART T-LIGHT HOLDER|536365           |\n",
      "|{WHITE METAL LANTERN, 536365}               |WHITE METAL LANTERN               |536365           |\n",
      "+--------------------------------------------+----------------------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------------+---------+\n",
      "|Description                       |InvoiceNo|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |\n",
      "|WHITE METAL LANTERN               |536365   |\n",
      "+----------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from complexDF dataframe select col complex,col complex of description,col complex of InvoiceNo with getfield\n",
    "#select complex data in different columns like description and invoiceno\n",
    "complexDF.select(col(\"complex\"),col(\"complex\").getField(\"Description\"),col(\"complex\").getField(\"InvoiceNo\")).show(2,False)\n",
    "complexDF.select(\"complex.Description\").show(2,False)\n",
    "complexDF.select(\"complex.*\").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6f286bb-7cea-448a-ac52-c852ffd73c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------------+----------------------------------------+\n",
      "|Description                       |array_col                               |\n",
      "+----------------------------------+----------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|WHITE METAL LANTERN               |[WHITE, METAL, LANTERN]                 |\n",
      "+----------------------------------+----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|WHITE       |\n",
      "|WHITE       |\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the split function, with retail_data select description and \n",
    "#for every space split the description and rename new column as array_col\n",
    "#for every space split the description and rename new column as array_col and select only 0th index values of array_col column\n",
    "from pyspark.sql.functions import split\n",
    "retail_data.show(2)\n",
    "retail_data.select(\"Description\",split(\"Description\",\" \").alias(\"array_col\")).show(2,False)\n",
    "retail_data.select(\"Description\",split(\"Description\",\" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a77af6b4-98c2-4466-aa23-1f25c717885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+---------+\n",
      "|Description                       |new                                     |size(new)|\n",
      "+----------------------------------+----------------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|5        |\n",
      "|WHITE METAL LANTERN               |[WHITE, METAL, LANTERN]                 |3        |\n",
      "+----------------------------------+----------------------------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------------------+-------------------------------+\n",
      "|Description                       |size(split(Description,  , -1))|\n",
      "+----------------------------------+-------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|5                              |\n",
      "|WHITE METAL LANTERN               |3                              |\n",
      "+----------------------------------+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the size function, with retail_data select description and \n",
    "#check the size after spliting the col description with \" \"\n",
    "from pyspark.sql.functions import size\n",
    "retail_data.select(\"Description\",split(\"Description\",\" \").alias(\"new\")).select(\"Description\",\"new\",size(\"new\")).show(2,False)\n",
    "\n",
    "retail_data.select(\"Description\",size(split(\"Description\",\" \"))).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c41a4ef-007d-4b32-8a7f-56f2d0ec1bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------------------------------------------+\n",
      "|Description                       |array_contains(split(Description,  , -1), WHITE)|\n",
      "+----------------------------------+------------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|true                                            |\n",
      "|WHITE METAL LANTERN               |true                                            |\n",
      "+----------------------------------+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the array_contains function, with retail_data select description and \n",
    "#check the array_contains has white in description column\n",
    "from pyspark.sql.functions import array_contains\n",
    "retail_data.select(col(\"Description\"),array_contains(split(\"Description\",\" \"),\"WHITE\")).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "221f9abe-1a86-45a2-a1b4-f89701047ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "|WHITE METAL LANTERN               |536365   |WHITE   |\n",
      "|WHITE METAL LANTERN               |536365   |METAL   |\n",
      "|WHITE METAL LANTERN               |536365   |LANTERN |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365   |CREAM   |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365   |CUPID   |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the explode function, with retail_data add new column \"splitted\" contains splitting of description with \" \"\n",
    "#and add another new column \"exploded\" contains explod of splitted column\n",
    "#and select columns \"Description\", \"InvoiceNo\", \"exploded\"\n",
    "from pyspark.sql.functions import explode\n",
    "retail_data.withColumn(\"splitted\",split(\"Description\",\" \")).withColumn(\"exploded\",explode(\"splitted\")).select( \"Description\", \"InvoiceNo\", \"exploded\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4a3d8f94-1d31-44e6-9444-c54cdd159f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|complex_map                                   |\n",
      "+----------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365}|\n",
      "|{WHITE METAL LANTERN -> 536365}               |\n",
      "+----------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|NULL                            |\n",
      "|536365                          |\n",
      "|NULL                            |\n",
      "|NULL                            |\n",
      "|NULL                            |\n",
      "+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------------------+------+\n",
      "|key                                |value |\n",
      "+-----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365|\n",
      "|WHITE METAL LANTERN                |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365|\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365|\n",
      "+-----------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the create_map function, with retail_data select create_map of Description and InvoiceNo\n",
    "#rename it as complex_map\n",
    "#do the same as above but now extra select like new_col[some value]\n",
    "#do the same as above but explode the complex_map column\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "retail_data.select(create_map(\"Description\",\"InvoiceNo\").alias(\"complex_map\")).show(2,False)\n",
    "retail_data.select(create_map(\"Description\",\"InvoiceNo\").alias(\"complex_map\")).selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(5,False)\n",
    "retail_data.select(create_map(\"Description\",\"InvoiceNo\").alias(\"complex_map\")).selectExpr(\"explode(complex_map)\").show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "053c5a76-8225-474f-b6df-e811a2675042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|jsonstring                         |\n",
      "+-----------------------------------+\n",
      "|{\"jsonkey\":{\"jsonvalue\":[1,2,3,4]}}|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in jsondf variable store the range(1) and contains value of '{\"jsonkey\":{\"jsonvalue\":[1,2,3,4]}}' as jsonstring\n",
    "\n",
    "jsondf=spark.range(1).selectExpr(\"\"\" '{\"jsonkey\":{\"jsonvalue\":[1,2,3,4]}}' as jsonstring\"\"\")\n",
    "jsondf.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b20ef7b-1241-4178-8008-d7860707a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the udf,col functions and longtype \n",
    "#in udfExampleDF create a df \"num\" contains 5 numbers\n",
    "#create a power function(power3) that return power of 3\n",
    "#convert the python function into udf and store in powering variable\n",
    "#using udfExampleDF dataframe pass the values to the power3 udf \n",
    "#register that power3 udf and using udfExampleDF dataframe with selectExpr call the udf\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import LongType\n",
    "udfExampleDF=spark.range(5).toDF(\"num\")\n",
    "def power3(val):\n",
    "    return val**3\n",
    "power3(2)\n",
    "powering=udf(power3)\n",
    "udfExampleDF.select(powering(col(\"num\"))).show(5)\n",
    "spark.udf.register(\"power3\",power3,LongType())\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceae1a5-d612-4b4f-b192-552fcbaa2c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
