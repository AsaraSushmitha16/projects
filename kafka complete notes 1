zookeeper-server-start zookeeper.properties
kafka-server-start server.properties

kafka-topics --create --topic hello-world --bootstrap-server localhost:9092 partitions 1 

kafka-console-producer --topic hello-world --bootstrap-server localhost:9092
kafka-console-consumer --topic hello-world --from-beginning --bootstrap-server localhost:9092 

--------------------------------------------------------------------------------------------------------------------------------------------------------
"MULTIPLE PRODUCER & MULTIPLE CONSUMER IN A KAFKA TOPIC"

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello-world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1 

starting the first producer: kafka-console-producer --topic hello-world --bootstrap-server localhost:9092
starting the first consumer: kafka-console-consumer --topic hello-world --from-beginning --bootstrap-server localhost:9092

in producer1 we can produce the messages and consumer1 will consume the messages from beginning
will start consumer2:  kafka-console-consumer --topic hello-world --bootstrap-server localhost:9092
will read the messages from producer1 not from beginning

will start producer2: kafka-console-producer --topic hello-world --bootstrap-server localhost:9092

now from both the producers we are sending msgs and both consumer consuming the msgs
if one of the producer down also, consumer will read msgs from other


--------------------------------------------------------------------------------------------------------------------------------------------------------
"TOPIC WITH MULTIPLE PARTITIONS"
#In Apache Kafka, a topic is a stream of data similar to a table in a database, 
where data produced by producers is stored and later consumed by consumers. Each topic is split into multiple partitions, 
which enable Kafka to scale horizontally and allow parallel processing. Messages sent to a topic are distributed across these partitions, 
either randomly or based on a key. Within each partition, messages are assigned a unique, sequential offset, 
starting from 0 and incrementing with each new message. Offsets are only meaningful within the context 
of a specific partition and are used by consumers to track message positions. While message ordering is guaranteed within a single partition, 
there is no ordering guarantee across partitions. Kafka messages are immutable and retained only for a configured period (e.g., 1 week),
after which they are purged—but their offset values continue to increase. Understanding the role of topics, partitions, and offsets is essential 
for designing robust Kafka-based systems.



starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 3 

running the python code round-robin-for-topic-with-multiple-partitions.py for roundrobin algorithm
while running it will go the specific partition based on hash algorithm

starting the consumer: kafka-console-consumer --topic hello-world --from-beginning --bootstrap-server localhost:9092
will read the messages from beginning but not in the specific order because the number of partitions are greater than 1

stored messages like below in server
partition 0: message1,message4,message7,message10
partition 1: message2,message5,message8,message11
partition 2: message3,message6,message9,message12

============

->reading the messages from specific partition:
in consumer we can use the command: kafka-console-consumer --topic hello-world --from-beginning --bootstrap-server localhost:9092 --partition 2

============


->reading the messages from specific partition and specific offset:
in consumer we can use the command: 
D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092 --partition 0 --offset 0
message1
message4
message7
message10
Processed a total of 4 messages

and
D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092 --partition 0 --offset 1
message4
message7
message10
Processed a total of 3 messages

from the offset it will read, not from the beginning values of offset


============

->reading the messages from specific offset:
D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092 --offset 0
The partition is required when offset is specified.


--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA CLUSTER WITH MULTIPLE BROKERS"

#When a Kafka topic is created with multiple partitions in a multi-server (multi-broker) cluster, 
Kafka automatically tries to distribute those partitions across different brokers rather than placing them all on a single server.
The reason is simple: fault tolerance and availability. If all partitions of a topic were located on one server and that server went down, 
then the entire topic would become inaccessible—producers couldn't publish messages, and consumers couldn't read any data.
However, if the partitions are spread across multiple servers, then if one broker fails, only the partitions on that server are temporarily unavailable, 
while the remaining partitions (on other brokers) can still accept new messages and serve consumers. 
This ensures that part of the topic remains operational even if a broker goes offline, 
enhancing reliability and scalability in distributed Kafka deployments.


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 3 

running the python code round-robin-for-topic-with-multiple-partitions.py for roundrobin algorithm

while running it will go the specific partition based on hash algorithm

starting the consumer: kafka-console-consumer --topic hello-world --from-beginning --bootstrap-server localhost:9092
will read the messages from beginning but not in the specific order because the number of partitions are greater than 1

stored like below in server
partition 0: message1,message4,message7,message10
partition 1: message2,message5,message8,message11
partition 2: message3,message6,message9,message12

if we stop the server, then in consumer and producer shows error that WARN [Consumer clientId=console-consumer, groupId=console-consumer-30236] Connection to node 0 (CLAP2558.prodapt.com/192.168.56.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
we have only one server, if that is down then entire clsuter is down, so we are unable to read msgs from producer

============

->so starting the multiple servers with multiple partitions

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server0(broker0): kafka-server-start server0.properties
starting the server1(broker1): kafka-server-start server1.properties
starting the server2(broker2): kafka-server-start server2.properties

creating the topic: kafka-topics --create --topic hello_world1 --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 1 partitions 5 

server0:
partitions are:hello_world1-1,hello_world1-4
server1:
partitions are:hello_world1-0,hello_world1-3
server2:
partitions are::hello_world1-2

running the python code round-robin-for-topic-with-multiple-partitions.py for roundrobin algorithm

server0:
partitions are:
hello_world1-1 (messages are): message2,message7,message12
hello_world1-4 (messages are): message5,message10


server1:
partitions are:
hello_world1-0 (messages are): message1,message6,message11
hello_world1-3 (messages are): message4,message9


server2:
partitions are:
hello_world1-2 (messages are): message3,message8

if we stop the server0, then the messages in that specific server will not consumed by consumer.
but if we try to run the broker(server0) is up and running again, we can restart the consumer so messages can be restored.


--------------------------------------------------------------------------------------------------------------------------------------------------------
"TOPIC WITH REPLICATION IN MULTIPLE BROKERS"

#In Kafka, replication is a critical feature used to ensure high availability and fault tolerance. 
When you have multiple brokers (servers), Kafka automatically creates replicas of partitions across different brokers. 
For example, if Partition 0 and Partition 2 are on Broker 1, Kafka may replicate those partitions to Broker 2 or Broker 3. 
This means that if Broker 1 fails, Kafka can still serve requests using the replica partitions on the other brokers. 
Both consumers and producers can interact with these replicas through leader election, where one replica becomes the active leader. 
Replication happens at the partition level, not the individual message level, meaning that entire partitions are copied to other brokers. 
This replication ensures that even if a broker goes down, the data remains accessible and 
the system continues functioning without data loss. Kafka manages which broker holds the leader and followers of each partition, 
making the process seamless and reliable.

#In Kafka, each partition has a leader that handles all read and write requests for that partition. 
The other replicas of the same partition that stay in sync with the leader are called in-sync replicas (ISR).
These ISRs reside on different brokers and continuously replicate the data from the leader. 
If a broker hosting a leader partition goes down, Kafka automatically elects one of the in-sync replicas to become the new leader. 
This new leader then starts handling all client requests for that partition. 
This automatic leader failover mechanism ensures high availability and uninterrupted message flow, even if one broker fails, 
as long as there is at least one in-sync replica available.
->

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server0(broker0): kafka-server-start server0.properties
starting the server1(broker1): kafka-server-start server1.properties
starting the server2(broker2): kafka-server-start server2.properties

creating the topic: kafka-topics --create --topic hello_world1 --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 3 partitions 7 

server0:
partitions are:
hello_world1-0
hello_world1-1
hello_world1-2
hello_world1-3
hello_world1-4
hello_world1-5
hello_world1-6

server1:
partitions are:
hello_world1-0
hello_world1-1
hello_world1-2
hello_world1-3
hello_world1-4
hello_world1-5
hello_world1-6

server2:
partitions are:
hello_world1-0
hello_world1-1
hello_world1-2
hello_world1-3
hello_world1-4
hello_world1-5
hello_world1-6

D:\kafka\bin\windows\kafka-topics --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --describe --topic hello_world1

output:
Topic: hello_world1     TopicId: t-FoO5VrRLuCPBdDrA5FGA PartitionCount: 7       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: hello_world1     Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2                                               
        Topic: hello_world1     Partition: 1    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1                                               
        Topic: hello_world1     Partition: 2    Leader: 1       Replicas: 1,2,0 Isr: 1,2,0                                               
        Topic: hello_world1     Partition: 3    Leader: 0       Replicas: 0,2,1 Isr: 0,2,1                                               
        Topic: hello_world1     Partition: 4    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0                                               
        Topic: hello_world1     Partition: 5    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2
        Topic: hello_world1     Partition: 6    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2


running the python code round-robin-for-topic-with-multiple-partitions.py for roundrobin algorithm
messages are stored as below
server0:
partitions are:
hello_world1-0: message1,message8
hello_world1-1: message2,message9
hello_world1-2: message3,message10
hello_world1-3: message4,message11
hello_world1-4: message5,message12
hello_world1-5: message6
hello_world1-6: message7

server1:
partitions are:
hello_world1-0: message1,message8
hello_world1-1: message2,message9
hello_world1-2: message3,message10
hello_world1-3: message4,message11
hello_world1-4: message5,message12
hello_world1-5: message6
hello_world1-6: message7

server2:
partitions are:
hello_world1-0: message1,message8
hello_world1-1: message2,message9
hello_world1-2: message3,message10
hello_world1-3: message4,message11
hello_world1-4: message5,message12
hello_world1-5: message6
hello_world1-6: message7

if one of the server is down, remainin two will work and consumer can consume all the messages
if 2 of the servers are down...its the same
but the below will change
D:\kafka\bin\windows\kafka-topics --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --describe --topic hello_world1
Topic: hello_world1     TopicId: xbqwkduST1aebDN2dGBjfA PartitionCount: 7       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: hello_world1     Partition: 0    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1
        Topic: hello_world1     Partition: 1    Leader: 2       Replicas: 1,2,0 Isr: 2,0,1
        Topic: hello_world1     Partition: 2    Leader: 2       Replicas: 0,1,2 Isr: 2,0,1
        Topic: hello_world1     Partition: 3    Leader: 2       Replicas: 2,1,0 Isr: 2,0,1
        Topic: hello_world1     Partition: 4    Leader: 2       Replicas: 1,0,2 Isr: 2,0,1
        Topic: hello_world1     Partition: 5    Leader: 2       Replicas: 0,2,1 Isr: 2,0,1
        Topic: hello_world1     Partition: 6    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1

once the brokers down comes to up and running then again the leader will be asusal as before




--------------------------------------------------------------------------------------------------------------------------------------------------------
"IN-DEPTH INTUITION ON KAFKA RACK AWARENESS WITH PRACTICAL DEMO"
 __________                   
| broker0  |        
|          |
|          |
| broker1  |
|          |
|          |
|__________|
 rack1
 __________
| broker2  |
|          |
|          |
|          |
|          |
|          |
|__________|
rack2

each rack contains datanodes in hdfs, so same way in kafka there are racks inside that brokers will present
broker0,broker1 are present in rack1
broker2 is present in rack2
in case of data fail we are replicating the brokers in different rack
broker0,broker1,broker2 contains the same data
if rack1 is failed due to network issue then rack2 will used, for that reasonwe are making it replica in rack2 with same data

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server0(broker0): kafka-server-start server0.properties
starting the server1(broker1): kafka-server-start server1.properties
starting the server2(broker2): kafka-server-start server2.properties

creating the topic: kafka-topics --create --topic hello_world1 --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 2 partitions 1 

server0:
partitions are:hello_world1-0
server1:
partitions are:hello_world1-0
server2:
partitions are 0 because replication factor is 2, so kafka will not allocate the partition for 3rd server

checking how many brokers are running:  D:\kafka\bin\windows\zookeeper-shell localhost:2181 ls /brokers/ids

checking how many the leader and replicas allocated:
D:\kafka\bin\windows\kafka-topics --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --describe --topic hello_world1
it will allocate that leaders are 0,1 servers, but they both are in same rack1, so if rack1 fails we may lost the data

->setting the broker.rack:
in server0.properties and server1.properties set broker.rack=1
in server2.properties set broker.rack=2
so after this try again creating new topic 

creating the topic: kafka-topics --create --topic hello_world2 --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 2 partitions 1 

server0:
partitions are:hello_world2-0
server1:
partitions are:hello_world2-0
server2:
partitions are 0 because replication factor is 1, so kafka will not allocate the partition for 3rd server

checking how many brokers are running:  D:\kafka\bin\windows\zookeeper-shell localhost:2181 ls /brokers/ids
[0,1,2]
checking how many the leader and replicas allocated:
D:\kafka\bin\windows\kafka-topics --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --describe --topic hello_world2
it will allocate that leaders are 0,2 servers, so if rack1 fails we may not lost the data because rack2 is available



--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA LOG SEGMENTS IN-DEPTH INTUITION"
#In Apache Kafka, when producers send messages to a topic, those messages are stored in partitions as append-only log files. 
Instead of writing all messages into one large file, Kafka splits each partition’s data into smaller files called segments. 
This makes data management more efficient and allows for faster reads and log cleanup. 
The size of each segment is configurable—once a segment reaches a certain size, 
Kafka automatically creates a new one to continue storing incoming messages. 
Each topic can have multiple partitions, and these partitions are distributed across different brokers (servers) in the 
Kafka cluster to support parallel processing, scalability, and fault tolerance. For example, 
a topic called Topic A might have four partitions: Partition 0 to Partition 3. Each of these partitions contains multiple segment files 
where messages are stored in the order they arrive. Segments are the smallest unit of storage in Kafka. A group of segments forms a partition, 
a group of partitions forms a topic, and multiple topics are managed within a Kafka cluster. 
This layered architecture—segments → partitions → topics → cluster—is what enables Kafka to reliably and efficiently handle massive volumes of streaming data in real time.



topic--->partition--->offset

each partition are divided into offsets

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

in server.properties change the value log.segment.bytes to 1000 so the files are filled 1000bytes then another file will be created

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the program log_segments python file to generate 100 numbers and consumer starts consuming the messages from cluster

when we see the logs in specific partition there will be no.of files of txt which will be divided on 1000bytes


--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA PRODUCER KEY AND MESSAGE ACKNOWLEDGEMENTS"
#In a Kafka cluster, a topic can have multiple partitions, and when a producer sends messages to that topic, a key question arises: 
how are those messages distributed across the different partitions? Specifically, 
how does Kafka decide whether a message should go to Partition 0, Partition 1, or any other available partition? 
This distribution is handled using message keys. When a producer sends a message, it can optionally include a key along with the value. 
This key can be anything—like a string, a number, or any identifier. If no key is provided, Kafka treats the key as null. 
When a key is included, Kafka uses a hashing algorithm to determine which partition the message should go to. 
The key is passed as input to the hash function, and the output of that function decides the partition where the message will be stored. 
The output of the hash function always maps to one of the available partitions. 
For example, imagine a simple hashing strategy where the key is divided by 3, and the remainder determines the partition number. 
So, if a message has a key of 7, dividing it by 3 gives a remainder of 1, meaning Kafka would route that message to Partition 1. Similarly, 
if the key is 5, the remainder is 2, so the message would be sent to Partition 2. This ensures that messages with the same key always go to the same partition, 
maintaining order for that key. However, if the message key is set to null, Kafka uses a round-robin approach for partitioning. 
In this case, Kafka will send the first message to Partition 0, the second to Partition 1, the third to Partition 2, and so on, continuing in a circular fashion. 
This round-robin approach ensures that messages are evenly distributed across all partitions when no key is provided. 
If a key is included, Kafka applies the hashing algorithm to determine which partition the message will go to. 
An important point to note is that if multiple messages share the same key, they will always be routed to the same partition. 
For example, if several messages have the key "7" and the hashing strategy is to divide by 3 (and use the remainder), 
the result will always be the same—1. So, all messages with the key "7" will consistently be directed to Partition 1. 
This concept is widely used in partitioning, especially in large-scale systems, and should be familiar if you’ve worked with similar distributed systems. 
To summarize, messages are distributed across partitions based on their keys. If you're experimenting with this, 
you need to send key-value pairs instead of just values to demonstrate partitioning. Two important producer configurations 
to keep in mind are the sparse.key property, which should be set to true if you want to make the key mandatory, and the key separator, 
which defines how the key and value are separated (e.g., using a colon). When sending messages with key-value pairs, 
these settings ensure that Kafka correctly handles the distribution and separation of data.


if key=0, the messages are stored in the roundrobin way
if key=1, the messages are sent to the specific partition
for that 2 properties are required, parse.key=true and key.seperator=:

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2

start the producer: kafka-console-producer --bootstrap-server localhost:9092 --property 'parse.key=true' --property 'key.seperator=:' --topic hello-world 
>101:'hi'
>102:'hello'
>103:'good'
>101:'well'
>102:'hey'
>101:'iam'
so the messages under 102 goes to partition2 and other will go to partition1

-->
ack=0, after consuming the message from broker, consumer will not provide any acknowledgement to broker that recieved data
ack=1, after consuming the message from broker, consumer will provide acknowledgement to broker that recieved data
ack=all, after consuming the message from broker, consumer will provide acknowledgement to broker when all the leaders and in-sync replicas recieved the data(this have latency becoz all the leaders and in-sync replicas needs to get data and very safety)

#The acknowledgement parameter (acks) in Kafka is a crucial setting that determines how many partition 
replicas must confirm receipt of a message before the producer considers the write operation successful. 
When the producer writes data to a Kafka topic, that data may be replicated across multiple brokers (or nodes) to ensure fault tolerance. 
The acks setting controls how many of those replicas must successfully receive the message before an acknowledgment is sent back to the producer. 
This setting plays a significant role in determining message reliability—how likely it is that a message could be lost in case of a failure.

Let’s consider the scenario where the acknowledgement parameter is set to zero. 
In this case, the producer will not wait for any acknowledgment from the broker before assuming that the message has been successfully written. 
The producer sends the message, and if there are network issues or the message fails to be written to the Kafka cluster, 
the producer won’t be aware of it—it simply “fires and forgets.” While this might seem efficient, it is a risky approach, as the producer won't be 
notified if something goes wrong. If the broker goes offline, encounters an error, or fails to receive the message for any reason, the producer won’t know, 
and the message may be lost without any chance of recovery.

Another option for the acknowledgement parameter is ack=1. With this setting, 
the producer will receive a success response as soon as the leader partition receives the message. 
Once the leader partition accepts the message, it sends an acknowledgment back to the producer, confirming that the message was successfully written. 
After receiving this acknowledgment, the producer can proceed with other tasks or continue writing additional messages. 
However, if the message cannot be written to the leader partition—for instance, if the leader is overwhelmed or a new leader needs to be elected—the producer 
will receive an error response. This allows the producer to understand that something went wrong and retry sending the message, 
thus reducing the risk of data loss compared to ack=0, where the producer has no feedback.

In the ack=1 configuration, the producer sends the data to the leader partition of the topic. 
Once the leader receives and writes the message, it sends an acknowledgment back to the producer, confirming the message was 
successfully written. However, a potential issue arises if replication is enabled for the topic. 
In this case, the producer only receives an acknowledgment from the leader partition, not from the in-sync replicas (ISRs) on other brokers. 
This means the producer does not know whether the message has been properly replicated to other partitions across different servers. 
Since the producer doesn't wait for acknowledgments from these replicas, it cannot guarantee that the message has been safely replicated across all partitions, 
which could be a disadvantage of using ack=1 for data consistency.

The last option for the acknowledgement parameter is ack=all (or ack=-1). With this setting, the producer will only receive a success 
response once the message is successfully written not just to the leader partition, but also to all the in-sync replicas (ISRs). 
In this configuration, the producer sends the data to the leader partition, and the leader writes the message. Then, 
the leader forwards the message to all the in-sync replicas. These replicas write the data and send acknowledgments back to the leader. 
Once the leader collects acknowledgments from all the ISRs, it sends a final acknowledgment to the producer, confirming that the message has been 
safely written across all replicas. This provides the highest level of data consistency and reliability but can introduce more latency due to 
waiting for acknowledgments from multiple replicas.

Once the producer receives the acknowledgment that the message has been successfully written and 
replicated across all partitions, it proceeds with producing the next message. This is the core concept behind ack=all. 
As you can see, this approach significantly reduces the chance of message loss, providing a high level of safety and reliability. However, 
the trade-off is increased latency. The producer must wait for the message to be replicated not only to the leader but also to all in-sync replicas before 
receiving the final acknowledgment. This added latency can slow down the process, but it ensures greater data safety and consistency. In this configuration, 
both the leader and the replicas must acknowledge the message before the producer can move on.

--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA PRODUCER INTERNALS"
Kafka Producer Internals: In a Kafka producer setup, the application sends messages to the Kafka cluster using APIs (e.g., Java or Python). 
Each message contains a topic, a partition (which is generally calculated based on the message’s key), and the actual data. 
The key is used for partitioning via a hashing algorithm, and the value is the actual message content. 
The first step in the producer process is serialization, where both the key and value are converted into a byte array using a serializer, 
enabling the message to be sent over the network as binary data.

Partitioning and Buffering: Once serialized, the data is sent to the partitioner, which determines the partition within the topic where the message will be stored. 
Kafka uses a simple hashing mechanism on the key (if provided) to decide which partition to write to. 
After partitioning, the message isn’t sent directly to the Kafka cluster but rather to an internal buffer. Each partition has its own buffer, 
which accumulates messages for efficient handling. The buffer helps optimize input/output operations and compression, especially in scenarios like run-length encoding.

Batching Messages: Kafka doesn't send messages individually. Instead, it batches messages together from the buffer for efficient network use. 
The producer collects messages into batches, which are then sent to the Kafka cluster. The size of these batches can be controlled using configuration parameters 
such as linger.ms (linger time in milliseconds) and batch.size (maximum batch size in bytes). 
Kafka waits for either the specified time to elapse or the batch size to be reached before sending the batch.

Batching Control Parameters: The linger.ms parameter defines how long the producer waits before sending accumulated messages, and 
batch.size sets the maximum size for each batch. For example, if you set linger.ms to 5 milliseconds and batch.size to 5 MB, 
Kafka will either wait 5 milliseconds for more messages to accumulate or send the batch once it reaches 5 MB—whichever happens first. This helps 
balance between throughput and latency, ensuring messages are sent efficiently.

Retries and Record Metadata: If message writing to the Kafka cluster fails due to networking issues or other problems, 
Kafka doesn’t throw an immediate exception. Instead, it retries the operation based on the configured retry parameter. For instance, 
if retries are set to 5, Kafka will attempt to send the message 5 times before failing. Once the message is successfully written, Kafka sends metadata 
to the producer, including the partition, offset, and timestamp of the message, confirming the write operation.

Handling High Message Throughput: If the producer is producing messages at a very high rate, and the buffer size 
is insufficient (e.g., the buffer is set to 32 MB), the buffer can fill up quickly, preventing new messages from being written to the Kafka cluster. 
In such cases, the producer may need to increase the buffer size to allow for efficient message accumulation and avoid message loss. 
This type of issue can be resolved only with a clear understanding of Kafka's architecture and configuration parameters.
--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA TOPIC PARTITIONS AND PRODUCERS USING PYTHON"
with_partition.py file
1) using the single partition to send message
2) using key and value to send messages(messages which contain same keys will go to specific partitioner)
3) same as 2nd but we use key_serializer and value_serializer
4) using the function we are getting the partition and sending messages


--------------------------------------------------------------------------------------------------------------------------------------------------------
"BUFFER.MEMORY, MAX.BLOCK.MS & PRODUCER IO THREAD IN KAFKA"

producer-->broker-->topic-->partition-->buffer(record accumulator)-->I/O thread-->Kafka cluster

so buffer have 2 properties
1)buffer.memory --> default 25mb
2)max.block.ms -->milli seconds

the file of buffer size is based on the above 2 properties, if one of the condition satisfies it will send that batch of data to I/O thread
so we use flush() and close() if the msgs are still there in buffer file but didnt push to cluster


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the python code: flush_concept
which consumer consumes data till 997 but 998,999 will be stred in buffer which consumer cant consume those 2 records

so we use fluch() and close() to consume the all records


--------------------------------------------------------------------------------------------------------------------------------------------------------
"PRODUCER RETRIES & IDEMPOTENT PRODUCER IN KAFKA"
In Kafka, when idempotence is enabled, the producer ensures that duplicate messages are not written to the Kafka cluster in case of failures during the write operation. 
If a batch fails to be acknowledged, Kafka does not resend it, but instead sends an acknowledgement to the producer, ensuring that the batch is only written once. 
This prevents duplicacy and guarantees that each message is written exactly once. However, a potential problem arises when the producer is sending multiple batches to a single partition. 
If the first batch is successfully written, but the second batch fails due to a network or server issue, Kafka retries sending the second batch. 
Meanwhile, the third batch might be successfully written, causing a disruption in the order of messages within the partition. 
This could result in a sequence like "first batch, third batch, second batch," which violates the expected ordering.

To solve this issue, Kafka provides the max.in.flight.requests.per.connection configuration. 
This parameter controls how many requests Kafka can send before receiving an acknowledgment for the previous ones. 
If set to a higher value (e.g., 5), Kafka will send multiple batches simultaneously, potentially causing the message order to be disrupted during retries. 
However, if you set max.in.flight.requests.per.connection to 1, Kafka will only send one request at a time and wait for the acknowledgment before sending the next batch. 
This ensures that even during retries, Kafka will not send any new batches until the current batch has been fully processed, thus preserving the order of messages within a partition.

By enabling this setting, you're effectively making the producer's message writing operation synchronous. 
This means that Kafka will wait for the acknowledgment of the first batch before sending the second, thereby preventing potential message order issues that arise from retry attempts. 
While setting max.in.flight.requests.per.connection to 1 can slow down the writing process due to the enforced waiting period, it ensures that message order is strictly maintained, 
which is essential in scenarios where message sequencing is critical. 
Therefore, enabling idempotence and configuring the max.in.flight.requests.per.connection parameter to 1 provides the trade-off between safety and performance, ensuring data integrity and ordered delivery.

--------------------------------------------------------------------------------------------------------------------------------------------------------
"IN-DEPTH INTUITION ON DIFFERENT WAYS TO SEND MESSAGES TO KAFKA TOPIC USING PYTHON"

#1. Fire-and-Forget
Definition:
In fire-and-forget, the Kafka producer sends a message to the broker without waiting for any acknowledgment.
The producer assumes the message is delivered successfully and moves on.
Pros:
Fastest method (lowest latency).
No blocking or waiting.
Cons:
No guarantee the message was received.
If the broker is down or there’s a network issue, the message is lost.
Not reliable – useful only when occasional message loss is acceptable (e.g., logging or metrics).
2. Synchronous Send
Definition:
The producer sends the message and waits (blocks) until it gets an acknowledgment from the broker.
This is done using .get() on the Future returned by send().
Pros:
Reliable – the producer knows whether the message was successfully written.
Good for critical data where loss is not acceptable.
Cons:
Slower – producer is blocked until broker responds.
Not suitable for high-throughput, low-latency systems.
3. Asynchronous Send
Definition:
The producer sends the message and attaches a callback to handle the response (success or failure).
It doesn’t block the main thread and allows sending other messages in the meantime.
Pros:
Efficient and non-blocking.
Allows error handling with retries, logs, alerts, etc.
Ideal for high-throughput systems.
Cons:
More complex logic due to callbacks.
Messages are handled out of order, so tracking sequence requires extra care.

Kafka Acknowledgment Levels (acks config)
This affects all three modes:
acks=0 → Fire-and-forget behavior
acks=1 → Leader broker only needs to ack (risk of data loss if leader fails after ack)
acks=all → All replicas must ack (strongest durability)

1)FIRE AND FORGET:
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
start the consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092
run the python code: different_ways

the python script will run the messages and consumer consumes it
if the server is down, we can see error messages in consumer console but still the script producer will produce the messages
so this the concept of fire and forgot, producer produces the messages but does not care whether consumer consumers it or not and there is no acknowledgement sent by consumer that till here we recieved messages.
disadvantages: client does not know whether the messages are reached or not

2)synchronous send
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
start the consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092
run the python code: different_ways

the python script will run the messages and consumer consumes it
if the server is down, we can see error messages in consumer console and exception error in producer scirpt console
this means that its sending acknowledgement that broker having issue

disadvantages: as it is synchronous, it will produce the messages in order, but here timetaking process.each time it should wait for 10secs

3)asynchronous send(success)
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
start the consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092
run the python code: different_ways
the producer shows the success message after successfully consumed by consumer


4)asynchronous send(failure)
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
start the consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092
run the python code: different_ways
the producer shows the messages that are sent, now if broker go down then consumer shows error in console and producer also shows error in console that from where it got failed to send messages

--------------------------------------------------------------------------------------------------------------------------------------------------------
"STRATEGIES FOR KAFKA TOPIC PARTITIONING WHEN KEY=NULL"
#How Kafka Distributes Messages with a Null Key — Round-Robin vs. Sticky Partitioning
When a Kafka producer sends messages without a key (i.e., the message key is null), Kafka needs to decide how to distribute these messages across multiple partitions of a topic. Let's first understand the simplest approach Kafka could use: round-robin partitioning.

Round-Robin Partitioning (Simplest Approach)
In this method, Kafka distributes each incoming message to the next available partition in a cyclical manner. For example, if a topic has 5 partitions (P0 to P4) and the producer receives six messages (1 to 6), they are distributed like this:

Message 1 → Partition 0
Message 2 → Partition 1
Message 3 → Partition 2
Message 4 → Partition 3
Message 5 → Partition 4
Message 6 → back to Partition 0

This continues in a round-robin fashion.
While this method ensures an even distribution of messages across partitions, it comes with two main drawbacks:
Performance Overhead: Each message is written to a different partition, which increases CPU usage and latency.
Broker Load: The broker has to keep switching partitions for each message, adding overhead and reducing efficiency.

To address these inefficiencies, Kafka introduced an optimized method for keyless message distribution called Sticky Partitioning.
Sticky Partitioning (Optimized Approach)
In sticky partitioning, Kafka takes advantage of the producer's batching mechanism. Instead of assigning every individual message to a different partition, it groups multiple messages into a batch (controlled by batch.size or linger.ms) and then sends the entire batch to a single partition.

For example:
Messages 1, 2, 3 form a batch → sent to Partition 0
Messages 4, 5, 6 form the next batch → sent to Partition 1
This approach offers two major benefits:
Lower Latency: Writing a batch of messages to a single partition is faster than sending them to multiple partitions.
Reduced CPU Usage on Broker: Since all messages in a batch go to the same partition, the broker doesn't need to compute a new partition for each individual message, saving processing effort.
This behavior is called "sticky" because the producer "sticks" to a single partition for the entire batch duration. Once a batch is sent, the producer may choose a new partition for the next batch.
--------------------------------------------------------------------------------------------------------------------------------------------------------

"CONSUMER & CONSUMER GROUP IN KAFKA"
Kafka Producer to Consumer Flow – Serialization & Communication
In Apache Kafka, when a producer (written in Java, Python, etc.) sends data to a Kafka topic, the data first undergoes serialization. This is essential because Kafka transmits data across the network as bytes, so both the key and value of a message are converted to byte arrays. When a consumer receives the data from a topic, it must perform deserialization, which means converting the byte arrays back into the original data format. For this to work seamlessly, the consumer must be aware of the serialization format used by the producer (e.g., String, JSON, Avro, etc.).

Another key architectural point is that Kafka follows a pull model, not a push model. This means the consumer requests data from Kafka; Kafka does not automatically push messages to the consumer. This is a fundamental concept and often asked in interviews.

Ordered Consumption from Partitions
When a consumer reads messages from a partition, it does so in order of offsets, from low to high. For instance, if offsets 0, 1, 2, and 3 exist in a partition, the consumer must read offset 0 first, then 1, and so on—it cannot skip ahead. Kafka guarantees ordered consumption within a single partition, but not across partitions.

Need for Consumer Groups – Scaling Consumers
Suppose there are multiple producers sending high volumes of data into a Kafka topic with several partitions. Initially, one might use a single consumer to read the messages, perform validations, and store results into a database or another system. However, this single consumer can become a bottleneck if the rate of incoming messages exceeds its processing capability. Over time, it will fall behind and fail to keep up with the producers.

To resolve this, Kafka introduces the concept of a Consumer Group.

What is a Consumer Group?
A consumer group is a collection of consumers that share the work of consuming messages from a topic. Each consumer in the group is assigned one or more partitions, and no two consumers in the same group read the same partition. Kafka ensures load balancing and automatically performs rebalancing when consumers join or leave the group.

Example:
Suppose a topic has 5 partitions.
You create a consumer group with 3 consumers.
Kafka might assign:
Consumer 1 → Partition 0, Partition 1
Consumer 2 → Partition 2, Partition 4
Consumer 3 → Partition 3

If Consumer 3 fails, Kafka will rebalance, and another consumer will take over Partition 3.
Scaling Rule and Idle Consumers
Keep in mind: the number of consumers in a group should not exceed the number of partitions. If you have more consumers than partitions, the extra consumers will remain idle, as a partition can only be consumed by one consumer in a group at a time. So, if a topic has 4 partitions and you add 5 consumers in the group, the 5th consumer will be idle.

Multiple Consumer Groups for the Same Topic
Kafka also allows multiple consumer groups to consume data from the same topic. Each group operates independently, meaning:

Group A might be for real-time analytics.
Group B might be for storing the data in a data lake.
Each group maintains its own offset tracking and does not affect the other.
This is useful when different applications or services need to consume the same data stream but perform different operations on it.
--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA CONSUMER GROUPS CLI DEMO | KAFKA-PYTHON"



starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 3
start the first consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092
run the producer: consumer_group.py

consumer strats to consume the messages

start the second consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092

both consumer1 and consumer2 will start consuming the same messages

now stop both the consumers and start with below consumers

start the first consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092 --group --from-the-first-consumer
start the second consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092 --group --from-the-first-consumer

if one of the consumer consumes data then second consumer will not consume that means both consumer will not consume the same data in same group

start the third consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092 --group --from-the-first-consumer

as there are 3 partitions, so each partition data will go to each consumer

start the fourth consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092 --group --from-the-first-consumer

as each data goes to specific consumer so 4th consumer will sit idle and if one of the consumer stops consuming the data then 4th consumer will start consuming in the same consumer group



--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA CONSUMER USING PYTHON & CONCEPT OF OFFSET-COMMIT"

lets assume there are 4 partitions and 4 consumers
each partition data will be consumed by each Consumer
if one of the consumer goes down so now there are 3 consumers but 4 partitions
in that case
1st consumer--> 1st partition
2nd consumer--> 2nd partition
3rd consumer--> 3rd and 4th partition

but 3rd consumer dont know from where the data it has to read for 4th partition or till where the 4th consumer read 
so for that there is option called offset, offset will commit till where is consumer which is down have read
the offset is stored in __consumer_offset(internal topic)

there are 2 ways to read data from kafka
1) consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092
reads data from beginning

2) consumer:>D:\kafka\bin\windows\kafka-console-consumer --topic hello_world --bootstrap-server localhost:9092
reads data from at that particular time

auto_offset_reset='latest':

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world4 --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the producer: offset.py
run the consumer:>consumer_offset.py

the consumer runs data from at that particular time because its auto_offset_reset='latest'
if the produced messages 1...10,11,12
consumer is stoped at 10
now again if we start the consumer it will read from where it left of 11,12,13...
because its commied the offset


auto_offset_reset='earliest':

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world4 --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the producer: offset.py
run the consumer:>consumer_offset.py

the consumer runs data from first_message to the at that particular time because its auto_offset_reset='earliest'
if the produced messages 1...10,11,12
consumer is stoped at 10
now again if we start the consumer it will read from where it left of 10,11,12,13...
because its commied the offset it will read one message earlier




--------------------------------------------------------------------------------------------------------------------------------------------------------
"MANUAL OFFSET COMMITS & EXACTLY-ONCE PROCESSING IN KAFKA CONSUMER USING PYTHON"

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2
producer:run offset.py 
consumer1:exactly-once-consumer1.py
consumer2:exactly-once-consumer2.py

so the partitions are assigned to specific consumer, if 1 consumer down other will pick the data of that consumer
in exactly-once: no duplicates the data will be offset commited once its Processed,if system failure also it will not loose the data
 
--------------------------------------------------------------------------------------------------------------------------------------------------------
"UNDERSTANDING KAFKA PARTITION ASSIGNMENT STRATEGIES WITH IN-DEPTH INTUITION AND PRACTICAL USING PYTHON"

there are two ways that consumer consumes the data from partition
1)range partition (partition_assignment_strategy=[RangePartitionAssignor])
for example there are 6 partitions 0,1,2,3,4,5
consumer1 will read data from 0,1,2 partitions and consumer2 will read data from 3,4,5 partitions 
2)roundrobin partition (partition_assignment_strategy=[RoundRobinPartitionAssignor])
hash algorithm 
for example there are 6 partitions 0,1,2,3,4,5
consumer1 will read data from 0,2,4 partitions and consumer2 will read data from 1,3,5 partitions 


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2


producer:producer-with-partition.py
consumer1:consumer-with-partition1.py
consumer2:consumer-with-partition2.py


--------------------------------------------------------------------------------------------------------------------------------------------------------
"APACHE KAFKA CONSUMER LAG ANALYSIS IN-DEPTH INTUITION"


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2


producer:producer-with-partition.py
consumer1:consumer-lag1.py
consumer2:consumer-lag2.py

to check the lag:D:\kafka\bin\windows\kafka-consumer-groups --bootstrap-server localhost:9092 --group demo --describe

so if we start only first consumer we can see more lag
if we start the both the consumer the lag will be constant


--------------------------------------------------------------------------------------------------------------------------------------------------------
"why do .index & .timeindex files exist in the kafka-log directory?"

.index file:

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1


producer:indexing_example.py
we see that messages are stored in server_logs
there is a file called 00000000000000000000.index in server_logs

earlier it was very hard to read data for consumer where the offset is stored and which position the data is stored
for that we have index
index makes the searching alogithm is easy and uses binary-search

to check the offsets are stored in which position:
D:\kafka\bin\windows\kafka-run-class kafka.tools.DumpLogSegments --files D:\kafka_logs\server_logs\hello_world-0\00000000000000000000.index --deep-iteration --print-data-log

to check the properties of topic:
D:\kafka\bin\windows\kafka-configs --entity-type topics --entity-name hello_world --describe --all --bootstrap-server localhost:9092


.timeindex file:

to check the messages or offset in particular timestamp or from the timestamp:
D:\kafka\bin\windows\kafka-run-class kafka.tools.DumpLogSegments --files D:\kafka_logs\server_logs\hello_world-0\00000000000000000000.timeindex --deep-iteration --print-data-log


log file:
D:\kafka\bin\windows\kafka-run-class kafka.tools.DumpLogSegments --files D:\kafka_logs\server_logs\hello_world-0\00000000000000000000.log --deep-iteration --print-data-log

broker knows the specific offset is stored 
the offsets are stored in multiple files, each file is renamed to offset number

for example iam reading the log of 59th as below, so it contains 59-78 and other file start from 79-:

D:\kafka\bin\windows\kafka-run-class kafka.tools.DumpLogSegments --files D:\kafka_logs\server_logs\hello_world-0\00000000000000000059.log --deep-iteration --prin
t-data-log
Dumping D:\kafka_logs\server_logs\hello_world-0\00000000000000000059.log
Log starting offset: 59
baseOffset: 59 lastOffset: 67 count: 9 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonM
s: OptionalLong.empty position: 0 CreateTime: 1711006201455 size: 268 magic: 2 compresscodec: none crc: 255525087 isvalid: true
| offset: 59 CreateTime: 1711006201454 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1302}
| offset: 60 CreateTime: 1711006201454 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1303}
| offset: 61 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1304}
| offset: 62 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1305}
| offset: 63 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1306}
| offset: 64 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1307}
| offset: 65 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1308}
| offset: 66 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1309}
| offset: 67 CreateTime: 1711006201455 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1310}
baseOffset: 68 lastOffset: 75 count: 8 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonM
s: OptionalLong.empty position: 268 CreateTime: 1711006201459 size: 245 magic: 2 compresscodec: none crc: 2000192531 isvalid: true
| offset: 68 CreateTime: 1711006201458 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1311}
| offset: 69 CreateTime: 1711006201458 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1312}
| offset: 70 CreateTime: 1711006201459 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1313}
| offset: 71 CreateTime: 1711006201459 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1314}
| offset: 72 CreateTime: 1711006201459 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1315}
| offset: 73 CreateTime: 1711006201459 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1316}
| offset: 74 CreateTime: 1711006201459 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1317}
| offset: 75 CreateTime: 1711006201459 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1318}
baseOffset: 76 lastOffset: 78 count: 3 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonM
s: OptionalLong.empty position: 513 CreateTime: 1711006201462 size: 130 magic: 2 compresscodec: none crc: 1738895853 isvalid: true
| offset: 76 CreateTime: 1711006201461 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1319}
| offset: 77 CreateTime: 1711006201461 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1320}
| offset: 78 CreateTime: 1711006201462 keySize: -1 valueSize: 16 sequence: -1 headerKeys: [] payload: {"number": 1321}
PS D:\aa_kafka_python> 




--------------------------------------------------------------------------------------------------------------------------------------------------------
"Schema registry"

the producer produces the message and consumer consumes the messages
but whether its bad data(different datatype) also it produces and consumer consumes messages means if we expect integer format but produced string format still it will consumed by consumer

to maintain specific data format we use shcema registry

without schema registry:
source--->producer--->topicA-kafka broker-topicB--->consumer--->target

with schema registry:


(register schema for the very first time)
             ------------------------------------------------------------>schema registry--------------------------------
             |                ------------------------------------------------|    |----------------------- |            |
             |                |(schemaID)                                                                   |            |
          |-------------------|-------------------------|                                            |------|-----------|---------|
source--->|producer           |                         |                                            |     consumer     |         |
          |schema(id + data)<-|                         |------------->topicA-kafka broker-topicB--->|                <-|         |--------------------->target
          |avro serializer(it will                      |                                            |     schema(id+data)        |
          |serialize the data and also                  |                                            |     avro deserailizer      |    
          |checks the schema, if its expecting int      |                                            |----------------------------|
          |but coming data is string it will not accept)|                                             local cache for schemas(schemaid:schema)
          |---------------------------------------------|
           local cache for schemas(schemaid:schema)                                                    


schema registry lies outside of the kafka cluster

-in this scenario,the producer produces the messages and the schema of it, for the first time the schema will be registered in schema registry
inreturn it gives scheamid of the data.then the data is passed into topic,from topic the consumer will consumes the messages by deserailizing.
-while consumer consumes the data consumer will check the schemaid in schema registry, if the schema of data whether its data type is int or etc will check.
if its matched with schema registry then consumer consumes it.if not then it will throw error that expected not this type of data.
-if its not first time its senind the data to the producer then it will not go to the schema registry again.
instead it will take it from the local cache for schemas(schemaid:schema)
-this schema will be used in both producer and consumer

schema evolution:
updating in the existing schema or inserting new schema or deleting the schema
when a schema is first created, it gets a unique schema id and a version number
with time our schemas will evolve we add new changes and if changes are compatible we get a new schema id and our version number increments within existing schema subject.
1)forward compatibility:
update producer to v2 version of the schema and gradually update consumers to the v2 version.
producer writing with v2(new schema)------------->consumer reading with v1(old schema)
2)backward compatibility:
update all consumers to the v2 version of the schema and then update producer to v2 version.
producer writing with v1(old schema)------------->consumer reading with v2(new schema)


--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA CONNECT"

-from producer the data is reading from different sources like text file
-then producer produces the data to kafka
-from kafka the data will be further consumed by different consumers like postgress sql,snowflake,elastic search
-if we are not sure working with postgress or elastic search then there comes to picture of kafka connect
-if we use kafka connect we can pull the data from different sources and consumed by different consumer

workflow:
source system--->kafka connect source----------------->kafka------>kafka connect sink--------------->target system
                 (read & ingest into kafka cluster)                (write from kafka to non-kafka)

connector:
connector in kafka connect is a job that manages and coordinates the tasks.
it decides how to split the data-copying work between the tasks.

task:
its a piece of work that provides service to accomplish actual job.

connector divide the actual job into smaller pieces as tasks in order to have parallelism and scalable data copying with very little configuration.
job means read & ingest into kafka cluster or write from kafka to non-kafka

worker:
is the node that is running the connector and its tasks

-reading data from mysql table and storing in the kafka topic:
1)in ubuntu:
docker container exec -it mysql-db mysql -p
mysql:
create database(first_db);
use first_db;
select database(); --> it will show the currently used database
create table(employee) with schema and insert values to it -->https://www.kaggle.com/datasets/sprabhagaran/employee

2)in pycharm
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties
creating the topic: kafka-topics --create --topic employee --bootstrap-server localhost:9092 --replication-factor 1 partitions 1 

3)in cmd:
install the two packages: pip install mysql-connector-python, pip install kafka-python
once done get into python shell
run each line by line from the link https://github.com/sprabhagaran/Development/blob/master/mysql-kafka.py

4)in another cmd:
start the consumer-> u can able to see the messages which are stored in mysql table employee
starting the consumer: kafka-console-consumer --topic employee --from-beginning --bootstrap-server localhost:9092

-reading data from kafka topic and storing in the mysql table:
1)in pycharm
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties
creating the topic: kafka-topics --create --topic kafkaTopic --bootstrap-server localhost:9092 --replication-factor 1 partitions 1 

starting the first producer: kafka-console-producer --topic kafkaTopic --bootstrap-server localhost:9092

2)in ubuntu:
docker container exec -it mysql-db mysql -p
mysql:
create database(first_db);
use first_db;
select database(); --> it will show the currently used database
create table(kafkaTopic) with schema batchid int(primary key auto increment),deptname varchar,salary int

3)run python code in pycharm: from-kafka-to-mysql.py
give inputs:
maths,3000
sci,200

the given inputs are stored in mysql table



--------------------------------------------------------------------------------------------------------------------------------------------------------
"KAFKA REBALANCER LISTENER & IMPLEMENTING A DATABASE SINK FOR KAFKA"

source-->producer-->kafka cluster--->consumer--->database
                        |<---------------|
                         offset commiting

producer producer the data,consumer consumes the data and commits the offset in kafka cluster and stored the data in database
if after consumer consumes the data and the next second its down then commiting the offset will not happen,this may lead to duplicate data

instead we will commit the offset in database as below:

source-->producer-->kafka cluster--->database


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2


producer:producer-balencer.py
consumer1:consumer-rebalancer1.py

the 2 partitions are assigned to consumer1 and starts reading data

after a while if we start the consumer2
consumer2:consumer-rebalancer2.py
one of the partition is assigned to consumer2
so each partition is assigned to each consumer

if one of the consumer is down also rebalnce occurs and consumer which is up will be taken care of 2 partitions


--------------------------------------------------------------------------------------------------------------------------------------------------------
"ERROR HANDLING IN KAFKA PRODUCER IN-DEPTH INTUITION"
                                                                      +acks,
                                                                      +max_in_flight_requests_per_connection,
                                                                      +enable_idempotence
start-->key serializer -->partition calcultation-->batching records-->sending records---------------------------->duplicates
        value serializer                                                |                                              |no
                                                                        |                               persistent leader and replicas
                                                                        |                                              |
                                                                        |     no                 yes    +reties  no    |
                                                                        |<----------- timeout<----------retry<-------success
                                                                                         yes |          no |         |yes
                              <----error handler<-----------------------------------------------------------         |
                             |                                                                                       |
stop<------------------------------success handler<------------------------------------------------------------------|


if the producer is down and then it will try to retry the data to send and checks the timeout or not
if its yes it will show error message in console that issue with producer
in that case the messages are lost

architecture:
1)   
                                                    kafka cluster
client--->api----->kafka producer----x------------->topic
                    |        |      exception
                    |        |               
          kafkaconsumer      |----------------------retry-topic
                |                                        |
                |<---------------------------------------|

the producer sends the data to retry-topic which are failed to prodcue into actual topic
then from retry topic the kafka consumer consumes the data
so retry-topic lies in kafka cluster, if kafka cluster is down then there will no retry topic also

2)
                                                   kafka cluster
client--->api----->kafka producer----x------------->topic
                    |        |      exception
                    |        |               
          job scheduler      |--------database
                |                         |
                |<------------------------|

the producer sends the data to database which are failed to prodcue into actual topic
then from database the data is consumed by job scheduler(py,java) and produce to kafka producer.
so database doesnt lies in kafka cluster, if kafka cluster is down then there will no issue.



--------------------------------------------------------------------------------------------------------------------------------------------------------
"CONFLUENT REST PROXY FOR KAFKA IN DEPTH INTUITION"

non-java-producers--->kafka-cluster--->consumer
instead of using simple producer to produce messages we use postman to send messages and consumer consumes it
in ubuntu:
start CONFLUENT:
confluent local services start
in that we can able to see kafka rest which is running on 8082
start the consumer:kafka-console-consumer --topic hello_world --from-beginning --bootstrap-server localhost:9092

in postman:
post: http://localhost:8082/topics/hello_world
body: raw and json
{"records":[{"value":{"hey":"first"}}]}
headers:
add last one more
Content-Type:application/vnd.kafka.json.v2+json

click on send to post the data
we can able to see in ubuntu consumer the message

so we can send the link(http://localhost:8082/topics/hello_world) to client to send messages from whever ever they want to consumer


--------------------------------------------------------------------------------------------------------------------------------------------------------
"DEAD LETTER QUEUE or DEAD LETTER TOPIC"


to handle errors if the format is not correct we store the data in dead letter queue or dead letter topic
we can use dead letter topics data for further reference or for analysis

SCRIPT1:

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic demo --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
creating the topic: kafka-topics --create --topic dlq_topic --bootstrap-server localhost:9092 --replication-factor 1 partitions 1

running the python code: test_dlq.py
start the consumer by: 
D:\kafka\bin\windows\kafka-console-consumer --topic demo --from-beginning --bootstrap-server localhost:9092
D:\kafka\bin\windows\kafka-console-consumer --topic dlq_topic --from-beginning --bootstrap-server localhost:9092

here in this code if the format is not in json then that data will be sent to dlq_topic and demo topic
if the format is json then that data will be sent to only demo topic


SCRIPT2:

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic demo --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
creating the topic: kafka-topics --create --topic dlq_topic --bootstrap-server localhost:9092 --replication-factor 1 partitions 1

running the python code: test_1.py
start the consumer by: 
D:\kafka\bin\windows\kafka-console-consumer --topic demo --from-beginning --bootstrap-server localhost:9092
D:\kafka\bin\windows\kafka-console-consumer --topic dlq_topic --from-beginning --bootstrap-server localhost:9092

here in this code if the format is not in json then that data will be sent only to dlq_topic
if the format is json then that data will be sent to only demo topic



--------------------------------------------------------------------------------------------------------------------------------------------------------
"DATA STREAMING USING FAUST"


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the python script(proj.py): faust -A proj worker -l info
start the producer: D:\kafka\bin\windows\kafka-console-producer --topic hello_world --bootstrap-server localhost:9092
>this is the first message
>second one
>third one
>fourth one

the above messages are stored in logs
after this stop the script and again try to produce the messages in producer
>5th one
>6th one
>7th
>8th
the same messages
now again start the script using same command (proj.py): faust -A proj worker -l info

we can able to see the messages in terminal of script 



--------------------------------------------------------------------------------------------------------------------------------------------------------
"DATA STREAMING USING FAUST WITH COMPLEX DATA TYPES"

source-->input kafka topic--->faust---->output kafka topic

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the first topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
creating the second topic: kafka-topics --create --topic send_greetings --bootstrap-server localhost:9092 --replication-factor 1 partitions 1

run the python script(complex.py): faust -A complex worker -l info

start the consumer: D:\kafka\bin\windows\kafka-console-consumer --topic send_greetings --bootstrap-server localhost:9092
start the producer: D:\kafka\bin\windows\kafka-console-producer --topic hello_world --bootstrap-server localhost:9092
>{"from_name":"ABC","to_name":"DEF"}
>{"from_name":"123","to_name":"456"}
>{"from_name":"sush","to_name":"dolly"}

the above messages are stored in logs
after this stop the script and again try to produce the messages in producer
>{"from_name":"1","to_name":"2"}
>{"from_name":"3","to_name":"4"}
the same messages
now again start the script using same command (complex.py): faust -A complex worker -l info

we can able to see the messages in terminal of script and in consumer





--------------------------------------------------------------------------------------------------------------------------------------------------------
"UNDERSTNDING THAT KAFKA TOPIC PARTITIONS STILL DRIVE PARALLELISM IN FAUST"

source-->kafka topic-->faust
here faust acts like consumer, so we have seen earlier that there are if there are 2 partitions then we can extend consumer to 2


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2

run the python script(parallelism_consumer.py): faust -A parallelism_consumer worker -l info
run the python script(parallelism_producer.py): python3 parallelism_producer.py or rightclick and run

we can able to see the messages the producer are sending are showing in consumer faust
we can increse the no of consumer 
run the same python script(parallelism_consumer.py): faust -A parallelism_consumer worker -l info --web-port=6067

due to increse in consumer, we can see odd numbers data went to one of the consumer and even numbers data go to different consumer
if we stop first consumer, the other consumer consumes all the messages
if we again up and run the first consumer...same follows that odd numbers data went to one of the consumer and even numbers data go to different consumer


--------------------------------------------------------------------------------------------------------------------------------------------------------
"STORING THE DATA FROM FAUST INTO MYSQL TABLE"


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
creating the topic: kafka-topics --create --topic kafkaTopic --bootstrap-server localhost:9092 --replication-factor 1 partitions 1

1)
run the script:faust -A streamsqlproducer worker -l info


start the consumer: D:\kafka\bin\windows\kafka-console-consumer --topic kafkaTopic --bootstrap-server localhost:9092
start the producer: D:\kafka\bin\windows\kafka-console-producer --topic hello_world --bootstrap-server localhost:9092
>{"subject":"telugu","marks":25}
>{"subject":"telugu","marks":25}
>{"subject":"hindi","marks":30}
>{"subject":"english","marks":40}
>{"subject":"science","marks":50}

then the consumer consumes the data


2)in ubuntu:
docker container exec -it mysql-db mysql -p
mysql:
create database(first_db);
use first_db;
select database(); --> it will show the currently used database
create table(kafkaTopic) with schema batchid int(primary key auto increment),subject varchar,marks int

run the script:topic-sql.py
this script will store the messages from kafkaTopic topic to kafkaTopic table


--------------------------------------------------------------------------------------------------------------------------------------------------------
"WORKING WITH TABLES IN FAUST"

1 example)

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the script streamtable.py: faust -A streamtable worker -l info


start the producer: D:\kafka\bin\windows\kafka-console-producer --topic hello_world --bootstrap-server localhost:9092
>{"name":"sush","age":45}
>{"name":"a","age":20}
>{"name":"b","age":30}
>{"name":"b","age":30}

the script streamtable.py will store the data in hello_world topic which age is greater than 30


2 example)

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world1 --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
run the script streamcount.py: faust -A streamcount worker -l info


start the producer: D:\kafka\bin\windows\kafka-console-producer --topic hello_world1 --bootstrap-server localhost:9092
>hello world
>hello kafka
>kafka is very popular hello world
>hi world
>hi there

the script streamcount.py will store the data in hello_world1 topic data count
┌Count Tabled─────┐
│ Key     │ Value │
├─────────┼───────┤
│ hello   │ 3     │
│ world   │ 3     │
│ kafka   │ 2     │
│ is      │ 1     │
│ very    │ 1     │
│ popular │ 1     │
│ hi      │ 2     │
│ there   │ 1     │
└─────────┴───────┘



--------------------------------------------------------------------------------------------------------------------------------------------------------
"CO-PARTITIONING KAFKA-TABLES IN FAUST APPLICATION"

1 example)

starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world --bootstrap-server localhost:9092 --replication-factor 1 partitions 2

start the consumer1: faust -A consumerstreamcountwithmorepartitions worker -l info
start the consumer2: faust -A consumerstreamcountwithmorepartitions worker -l info --web-port 6067
start the producer: producerstreamcountwithmorepartitions.py

we can see that 
the script consumerstreamcountwithmorepartitions will store the data in such a way that
first_consumer:
┌Count Tabled────┐
│ Key    │ Value │
├────────┼───────┤
│ Nepal  │ 3     │
│ India  │ 2     │
│ Bhutan │ 1     │
└────────┴───────┘
second_consumer:
┌Count Tabled────┐
│ Key    │ Value │
├────────┼───────┤
│ USA    │ 2     │
│ Bhutan │ 2     │
└────────┴───────┘
but actual ans was:
usa-2
nepal-3
bhutan-3
india-2
so bhutan is read by 2 partitions so first partition takes 2 data and other partition 1 data

for this issue we can use below method:

2)
starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic hello_world1 --bootstrap-server localhost:9092 --replication-factor 1 partitions 2

start the consumer1: faust -A secondconsumer worker -l info
start the consumer2: faust -A secondconsumer -l info --web-port 6067
start the producer: secondproducer.py

we can see that 
the script secondconsumer will store the data in such a way that
first_consumer:
┌Count Tabled────┐
│ Key    │ Value │
├────────┼───────┤
│ Bhutan │ 3     │
└────────┴───────┘

second_consumer:
┌Count Tabled───┐
│ Key   │ Value │
├───────┼───────┤
│ India │ 3     │
│ Nepal │ 4     │
└───────┴───────┘
so consumer reads data by country


--------------------------------------------------------------------------------------------------------------------------------------------------------
"WINDOWING IN KAFKA STREAMS USING FAUST FRAMEWORK IN PYTHON | TUMBLING WINDOW"

example 1)

time          9:00    9:01     9:02      9:03        9:04    9:05      9:06  
                |       |       |         |           |       |         |
data stream     | 5,3,8 | 9,2,8 |  5,7,1  |  5,3,2,6  |  5,3  |  1,8,9  |
total =            16       19       13         16        8        18



example 2)

time          9:00    9:01     9:02      9:03        9:04    9:05      9:06  
                |       |       |         |           |       |         |
data stream     | 5,3,8 | 9,2,8 |  5,7,1  |  5,3,2,6  |  5,3  |  1,8,9  |
total=           5,8,16  9,11,19  5,12,13   5,8,10,16    5,8     1,9,18

TUMBLING window means the total time spent in 1 min or particular time period


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic car_speed --bootstrap-server localhost:9092 --replication-factor 1 partitions 1
start the consumer(tumblingconsumer.py): faust -A tumblingconsumer worker -l info
start the producer(tumblingwindow.py): run the script tumblingwindow.py

the output in consumer terminal shows the total speed as example2





--------------------------------------------------------------------------------------------------------------------------------------------------------
"OUT OF ORDER / LATE ARRIVING DATA EVENTS HANDLING IN FAUST APPLICATION"


time          9:00    9:01     9:02      9:03        9:04    9:05      9:06  
                |       |       |         |           |       |         |
data stream     | 5,3,8 | 9,2,8 |  5,7,1  |  5,3,2,6  |  5,3  |  1,8,9  |
total=           5,8,16  9,11,19  5,12,13   5,8,10,16    5,8     1,9,18

after this inteval 9:06 we got the data from producer which should be sent in between 9:01 to 9:02
but due to producer is down got delay in data 
so in order to store the data in same interval...we should use memory to save every data of time and data stream
so if its delayed also we can store the delayed data in exact timestamp list




--------------------------------------------------------------------------------------------------------------------------------------------------------
"HOW TO CHOOSE THE NO.OF PARTITIONS FOR A KAFKA TOPIC? HORIZONTAL SCALING FOR KAFKA CONSUMER"        

1 case)       1000/sec
producer--->kafka cluster--->consumer1        consumer2        
                             100 msgs/sec     100 msgs/sec
        
        1000/100 =10 partitions required
        but for safety purpose we add 2 more partitions that is 10+2=12 partitions

2 case)
            30k/sec
producer--->kafka cluster--->consumer1 
                             10 million/sec

one single consumer is more than sufficient to process the 30k partitions because it has the capability of processing 10 million/sec so one partitions is more than enough

but single broker using to handle the load is not a good idea, atleast we should have 3 brokers
so each broker will handle the load 

3 case)

            1lak/sec
producer--->kafka cluster--->consumer1 
                             100/sec
        100000/100=1000 consumers
        so 1000 consumers is not a good idea because consumer should read data in leader and following replics,so this may lead lag 
        so based on that we should increase the partition to handle that by increasing the partitions


starting the zookeeper: zookeeper-server-start zookeeper.properties
starting the server(broker): kafka-server-start server.properties

creating the topic: kafka-topics --create --topic noofpartition --bootstrap-server localhost:9092 --replication-factor 1 partitions 1

start the consumer1: one_consumernoofpartitions.py
start the consumer2: two_consumernoofpartitions.py
start the producer: producernoofpartitions.py

at first due to single partition, only one of the consumer will be revoked and assigned
to handle the load we can increase the partition
increase the partition for existing topic : D:\kafka\bin\windows\kafka-topics --bootstrap-server localhost:9092 --alter --topic noofpartition --partitions 2

so now both the consumers in same consumer group will handle the load
