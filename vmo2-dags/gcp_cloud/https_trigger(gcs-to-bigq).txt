import logging
from google.cloud import storage
from google.cloud import bigquery
 
from google.cloud.bigquery import SchemaField
 
# Instantiate the storage client
storage_client = storage.Client()
bigquery_client = bigquery.Client()
 
def hello_world_check(request):
    """HTTP Cloud Function."""
    try:
        a=process_files()
        return a
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        return 'Error occurred'
def process_files():
    """Process files from a specific Cloud Storage bucket."""
    try:
        bucket_name = "oracle_error_files"
        bucket = storage_client.bucket(bucket_name)
        blobs = list(bucket.list_blobs())  # Convert iterator to list
        result = []
        for blob in blobs:
            if blob.name.endswith('.csv'):
                blob_info = {
                    "name": blob.name,
                    "size": blob.size,
                    "updated": blob.updated.isoformat(),
                    # Add more properties as needed
                }
                content = blob.download_as_string().decode('utf-8').split('\n')
                rows = [row.split(',') for row in content if row]  # Split rows by comma and exclude empty rows
                result.extend(rows[1:])  # Exclude the first row (header) and add remaining rows to result
        logging.info("All files processed.")
        b=insert_into_bigquery(result)
        return b
    except Exception as e:
        logging.error(f"Error processing files: {e}")
def insert_into_bigquery(data):
    """Insert data into BigQuery."""
    try:
        project_id = 'dataflow-poc-394712'
        dataset_id = 'vmo2_poc'
        table_id = 'check_status'
        dataset_ref = bigquery_client.dataset(dataset_id, project=project_id)
        table_ref = dataset_ref.table(table_id)
        table = bigquery_client.get_table(table_ref)
 
       
        errors = bigquery_client.insert_rows(table_ref, data, selected_fields=table.schema)
        if errors:
            logging.error(f"Errors inserting rows: {errors}")
            return "not passed"
        else:
            logging.info("Data inserted into BigQuery successfully.")
            return "passed"
    except Exception as e:
        logging.error(f"Error inserting data into BigQuery: {e}")
        return "failed"