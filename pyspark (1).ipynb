{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3caca076-6a79-4a41-b026-3a1b0077790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a sparksession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972e85cc-1c06-49dd-a023-ae70c127a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sparksession and store in spark variable\n",
    "spark=SparkSession.builder \\\n",
    ".appName(\"test\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe30c07-e470-4f57-bbbb-7c930c2822dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a df with 10 numbers\n",
    "number=spark.range(10).toDF(\"numbers\")\n",
    "number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877a9ea6-63ef-48a1-af47-f99fb8502fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      2|\n",
      "|      4|\n",
      "|      6|\n",
      "|      8|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from df take only divisibile_by_2 numbers and store in divisibile_by_2 variable\n",
    "divisible_by_2=number.where(\"numbers%2=0\")\n",
    "divisible_by_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f5215a-f1a4-4c61-9387-d2e064d311b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(numbers=0), Row(numbers=3), Row(numbers=6), Row(numbers=9)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from df take only divisibile_by_3 numbers and store in divisibile_by_3 variable, do collect,take,show on divisibile_by_3 variable\n",
    "divisible_by_3=number.where(\"numbers%3=0\")\n",
    "# divisible_by_3.show()\n",
    "# divisible_by_3.count()\n",
    "divisible_by_3.collect()\n",
    "divisible_by_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3a9f9f-e947-492c-a28b-e302b8dd49a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and show the output\n",
    "csv_load=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "csv_load.show()\n",
    "csv_load.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd17bd21-86e2-45aa-9d2f-24595157f6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Estonia|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|              Zambia|      United States|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "|       United States|           Bulgaria|    1|\n",
      "|       United States|            Georgia|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#76 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#76 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=118]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#74,ORIGIN_COUNTRY_NAME#75,count#76] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and sort the data with count column and show the output\n",
    "csv_load=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "sort_count=csv_load.sort(\"count\")\n",
    "sort_count.show()\n",
    "sort_count.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceac4cef-b0af-4509-8c5f-fbe93971e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\"\n",
    "csv_load=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "csv_load.createOrReplaceTempView(\"sql_csv_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddb23a2-f7bd-4bb8-91f6-59e12d174bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|            Anguilla|       1|\n",
      "|              Russia|       1|\n",
      "|            Paraguay|       1|\n",
      "|             Senegal|       1|\n",
      "|              Sweden|       1|\n",
      "|            Kiribati|       1|\n",
      "|              Guyana|       1|\n",
      "|         Philippines|       1|\n",
      "|            Djibouti|       1|\n",
      "|            Malaysia|       1|\n",
      "|           Singapore|       1|\n",
      "|                Fiji|       1|\n",
      "|              Turkey|       1|\n",
      "|                Iraq|       1|\n",
      "|             Germany|       1|\n",
      "|              Jordan|       1|\n",
      "|               Palau|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|              France|       1|\n",
      "|              Greece|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\",\n",
    "#write a query in sql with grouping DEST_COUNTRY_NAME having count(1),stroing the data in sql_data variable\n",
    "sql_data=spark.sql(\"\"\" select DEST_COUNTRY_NAME,count(1) from sql_csv_table where group by DEST_COUNTRY_NAME\"\"\")\n",
    "sql_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdffa8c9-2d9f-4fb5-8823-cb8b9629aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#114], functions=[count(1)])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 0\n",
      "         +- Exchange hashpartitioning(DEST_COUNTRY_NAME#114, 200), ENSURE_REQUIREMENTS, [plan_id=227]\n",
      "            +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#114], functions=[partial_count(1)])\n",
      "               +- FileScan csv [DEST_COUNTRY_NAME#114] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[DEST_COUNTRY_NAME#114], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#114, 200), ENSURE_REQUIREMENTS, [plan_id=217]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#114], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#114] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#153], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#153, 200), ENSURE_REQUIREMENTS, [plan_id=298]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#153], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#153] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\",\n",
    "# write a query in sql with grouping DEST_COUNTRY_NAME having count(1),stroing the data in sql_data variable\n",
    "#==========\n",
    "#load the csv file and store it in csv_load variable, select the column DEST_COUNTRY_NAME having count from aesc store in sql_with_python variable\n",
    "#explain the both\n",
    "csv_load=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "sql_data=spark.sql(\"\"\" select DEST_COUNTRY_NAME,count(1) as count from sql_csv_table group by DEST_COUNTRY_NAME\"\"\")\n",
    "sql_data.tail(5)\n",
    "sql_with_python=csv_load.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "sql_with_python.take(5)\n",
    "sql_data.explain()\n",
    "sql_with_python.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317867cb-420a-4f5c-84fd-9c57ee5a53f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\",\n",
    "# write a query in sql with max count column,stroing the data in sql_data variable\n",
    "#==========\n",
    "#load the csv file and store it in csv_load variable, select max of the count column store the result in pyspark_sql variable\n",
    "from pyspark.sql.functions import max\n",
    "sql_data=spark.sql(\"\"\" select max(count) from sql_csv_table \"\"\")\n",
    "sql_data.show()\n",
    "pyspark_sql=csv_load.select(max(\"count\"))\n",
    "pyspark_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9149187a-e48f-407c-a61b-787f82d9b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "| DEST_COUNTRY_NAME|destination|\n",
      "+------------------+-----------+\n",
      "|     United States|     411352|\n",
      "|            Canada|       8399|\n",
      "|            Mexico|       7140|\n",
      "|    United Kingdom|       2025|\n",
      "|             Japan|       1548|\n",
      "|           Germany|       1468|\n",
      "|Dominican Republic|       1353|\n",
      "|       South Korea|       1048|\n",
      "|       The Bahamas|        955|\n",
      "|            France|        935|\n",
      "|          Colombia|        873|\n",
      "|            Brazil|        853|\n",
      "|       Netherlands|        776|\n",
      "|             China|        772|\n",
      "|           Jamaica|        666|\n",
      "|        Costa Rica|        588|\n",
      "|       El Salvador|        561|\n",
      "|            Panama|        510|\n",
      "|              Cuba|        466|\n",
      "|             Spain|        420|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable and create a temporary table \"sql_csv_table\",\n",
    "#write a query in sql selecting DEST_COUNTRY_NAME and sum of count of grouping DEST_COUNTRY_NAME as destination,show the result in desc\n",
    "#store the result in csv_sql variable\n",
    "csv_load=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "csv_load.createOrReplaceTempView(\"sql_csv_table\")\n",
    "csv_sql=spark.sql(\"\"\" select DEST_COUNTRY_NAME, sum(count) as destination from sql_csv_table group by DEST_COUNTRY_NAME order by destination desc\"\"\")\n",
    "csv_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f5bf23-7d40-4545-a442-c6eafac183d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "| DEST_COUNTRY_NAME|destination|\n",
      "+------------------+-----------+\n",
      "|     United States|     411352|\n",
      "|            Canada|       8399|\n",
      "|            Mexico|       7140|\n",
      "|    United Kingdom|       2025|\n",
      "|             Japan|       1548|\n",
      "|           Germany|       1468|\n",
      "|Dominican Republic|       1353|\n",
      "|       South Korea|       1048|\n",
      "|       The Bahamas|        955|\n",
      "|            France|        935|\n",
      "|          Colombia|        873|\n",
      "|            Brazil|        853|\n",
      "|       Netherlands|        776|\n",
      "|             China|        772|\n",
      "|           Jamaica|        666|\n",
      "|        Costa Rica|        588|\n",
      "|       El Salvador|        561|\n",
      "|            Panama|        510|\n",
      "|              Cuba|        466|\n",
      "|             Spain|        420|\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv file and store it in csv_load variable\n",
    "#selecting DEST_COUNTRY_NAME and sum of count of grouping DEST_COUNTRY_NAME as destination,show the result in desc\n",
    "from pyspark.sql.functions import desc\n",
    "csv_load=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2015-summary.csv\")\n",
    "csv_pyspark=csv_load.groupBy(\"DEST_COUNTRY_NAME\") \\\n",
    ".sum(\"count\") \\\n",
    ".withColumnRenamed(\"sum(count)\",\"destination\") \\\n",
    ".sort(desc(\"destination\"))\n",
    "csv_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b7c3a-df30-4641-b0a6-0391767cf52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "191051c8-4661-4fda-8f7b-a045278eadcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', IntegerType(), True), StructField('InvoiceDate', TimestampType(), True), StructField('UnitPrice', DoubleType(), True), StructField('CustomerID', DoubleType(), True), StructField('Country', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#load the by_day folder and store in load_byday_csv and create the sql temp table as retail_data, assign the schema of load_byday_csv to schema\n",
    "#and print the schema\n",
    "load_byday_csv=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day\")\n",
    "load_byday_csv.createOrReplaceTempView(\"retail_data\")\n",
    "schema=load_byday_csv.schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adecab92-bb73-4361-bdb9-cc5e43c004eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerID|              window|        total_cost|\n",
      "+----------+--------------------+------------------+\n",
      "|   17450.0|{2011-09-20 00:00...|          71601.44|\n",
      "|      NULL|{2011-11-14 00:00...|          55316.08|\n",
      "|      NULL|{2011-11-07 00:00...|          42939.17|\n",
      "|      NULL|{2011-03-29 00:00...| 33521.39999999998|\n",
      "|      NULL|{2011-12-08 00:00...|31975.590000000007|\n",
      "|   18102.0|{2011-09-15 00:00...|31661.540000000005|\n",
      "|      NULL|{2010-12-21 00:00...|31347.479999999938|\n",
      "|   18102.0|{2011-10-21 00:00...|          29693.82|\n",
      "|   18102.0|{2010-12-07 00:00...|          25920.37|\n",
      "|   14646.0|{2011-10-20 00:00...|25833.559999999994|\n",
      "|      NULL|{2010-12-10 00:00...|25399.560000000012|\n",
      "|      NULL|{2010-12-17 00:00...|25371.769999999768|\n",
      "|      NULL|{2011-11-25 00:00...|24148.069999999992|\n",
      "|      NULL|{2011-11-29 00:00...|23744.250000000055|\n",
      "|   12415.0|{2011-06-15 00:00...| 23426.81000000001|\n",
      "|      NULL|{2010-12-06 00:00...|23395.099999999904|\n",
      "|      NULL|{2011-08-30 00:00...| 23032.59999999993|\n",
      "|      NULL|{2010-12-03 00:00...| 23021.99999999999|\n",
      "|   15749.0|{2011-01-11 00:00...|           22998.4|\n",
      "|   18102.0|{2011-10-03 00:00...|          22429.69|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the by_day folder and store in load_byday_csv and select CustomerID,UnitPrice * Quantity as total_cost,InvoiceDate\n",
    "#group by CustomerID with window function as 1 day, sum the total and rename it to total_cost and sort with desc of total_cost\n",
    "from pyspark.sql.functions import col,window,desc\n",
    "load_by_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "check_total=load_by_data.selectExpr(\"CustomerID\",\"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\") \\\n",
    ".groupBy(col(\"CustomerID\"),window(col(\"InvoiceDate\"),\"1 day\")) \\\n",
    ".sum(\"total_cost\") \\\n",
    ".withColumnRenamed(\"sum(total_cost)\",\"total_cost\") \\\n",
    ".sort(desc(\"total_cost\"))\n",
    "check_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a9fb036-d91e-4840-9ead-c9627d99c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', IntegerType(), True), StructField('InvoiceDate', TimestampType(), True), StructField('UnitPrice', DoubleType(), True), StructField('CustomerID', DoubleType(), True), StructField('Country', StringType(), True)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the csv files and store in load_by_data variable, store the schema in schema variable\n",
    "#read the stream by setting maxFilesPerTrigger as 1 and check if its streaming or not\n",
    "load_by_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "schema=load_by_data.schema\n",
    "print(schema)\n",
    "read_stream=spark.readStream.schema(schema).option(\"maxFilesPerTrigger\",1).option(\"header\",\"true\").format(\"csv\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "read_stream.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5501f347-7406-4e54-9f1e-77cd43dee451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv files and store in load_by_data variable, store the schema in schema variable\n",
    "#read the stream by setting maxFilesPerTrigger as 1 and check if its streaming or not in read_stream variable\n",
    "#with read_stream select CustomerID,UnitPrice * Quantity as total_cost,InvoiceDate\n",
    "#group by CustomerID with window function as 1 day, sum the total_cost store the entire result in purchasebycustomerperhour\n",
    "from pyspark.sql.functions import col,window\n",
    "load_by_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "schema=load_by_data.schema\n",
    "read_stream=spark.readStream.schema(schema).option(\"maxFilesPerTrigger\",1).option(\"header\",\"true\").format(\"csv\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "purchasebycustomerperhour=read_stream.selectExpr(\"CustomerID\",\"Quantity*UnitPrice as total_cost\",\"InvoiceDate\") \\\n",
    ".groupBy(\"CustomerID\",window(col(\"InvoiceDate\"),\"1 day\")) \\\n",
    ".sum(\"total_cost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a52a2ab-6399-4668-b821-67632d23b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the write_stream and with purchasebycustomerperhour write the stream,store in memory,and tablename is customer_purchases, output the mode as complete and start\n",
    "write_stream=purchasebycustomerperhour.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389548cc-7142-4846-b4e6-6732aba69960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerID|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select all the columns with sql in customer_purchases table and store in customer variable\n",
    "customer=spark.sql(\"\"\" select * from customer_purchases\"\"\")\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3aff82e-1854-4b67-81d7-098118b21295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f3dcd5e1f90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the csv files and store in load_by_data variable, store the schema in schema variable\n",
    "#read the stream by setting maxFilesPerTrigger as 1 and check if its streaming or not in read_stream variable\n",
    "#with read_stream select CustomerID,UnitPrice * Quantity as total_cost,InvoiceDate\n",
    "#group by CustomerID with window function as 1 day, sum the total_cost store the entire result in purchasebycustomerperhour\n",
    "#stop the write_stream and with purchasebycustomerperhour write the stream,store in console,and tablename is secondtb, output the mode as complete and start\n",
    "\n",
    "from pyspark.sql.functions import col,window\n",
    "load_by_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "schema=load_by_data.schema\n",
    "read_stream=spark.readStream.schema(schema).option(\"maxFilesPerTrigger\",1).option(\"header\",\"true\").format(\"csv\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "read_stream.isStreaming\n",
    "purchasebycustomerperhour=read_stream.selectExpr(\"CustomerID\",\"(UnitPrice*Quantity)as total_cost\",\"InvoiceDate\") \\\n",
    ".groupBy(\"CustomerID\",window(col(\"InvoiceDate\"),\"1 day\")) \\\n",
    ".sum(\"total_cost\")\n",
    "purchasebycustomerperhour.writeStream.format(\"console\").queryName(\"secondtb\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3c3e90b-ddef-46dd-93cb-d4cf922be87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Spark_files/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\n",
    "staticSchema=staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "840d36d0-5e25-492c-b055-420f00d7591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format,col\n",
    "some_df=staticDataFrame \\\n",
    ".na.fill(0) \\\n",
    ".withColumn(\"day_of_week\",date_format(col(\"InvoiceDate\"),\"EEEE\")) \\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d4f3b83-4f7d-43d8-8719-e489599aecf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444958"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataFrame=some_df.where(\"InvoiceDate < '2011-11-07'\")\n",
    "testDataFrame=some_df.where(\"InvoiceDate >= '2011-11-07'\")\n",
    "trainDataFrame.count()\n",
    "#testDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5f45aff-45e1-466a-a92f-a3ae36b3fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd_df=spark.sparkContext.parallelize([Row(value=1),Row(value=2),Row(value=3),Row(value=4)]).toDF()\n",
    "rdd_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b581186-ae56-44f9-876d-5aa71bd0b061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|(numbers + 10)|\n",
      "+--------------+\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "|            13|\n",
      "|            14|\n",
      "|            15|\n",
      "|            16|\n",
      "|            17|\n",
      "|            18|\n",
      "|            19|\n",
      "|            20|\n",
      "|            21|\n",
      "|            22|\n",
      "|            23|\n",
      "|            24|\n",
      "|            25|\n",
      "|            26|\n",
      "|            27|\n",
      "|            28|\n",
      "|            29|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe number_500 contains 500 numbers and add each value + 10 and show the result\n",
    "number_500=spark.range(500).toDF(\"numbers\")\n",
    "number_500.select(number_500[\"numbers\"]+10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a335153-59c0-4eb2-8c38-de7356d56579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=0), Row(id=1)]\n"
     ]
    }
   ],
   "source": [
    "#range of 2 numbers collect the numbers and store in rows_collect and print rows_collect\n",
    "rows_collect=spark.range(2).collect()\n",
    "print(rows_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7fdfa5f-c3f5-4b82-89e5-cba56ec19280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ByteType()\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "b=ByteType()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de1566ff-cf59-4208-932d-f07de705462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data and store in json_data variable and show the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.show()\n",
    "json_data.schema\n",
    "#json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7ac4eab-d1b7-4ab7-bd94-4b457742b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "|    1|\n",
      "|   62|\n",
      "|  588|\n",
      "|   40|\n",
      "|    1|\n",
      "|  325|\n",
      "|   39|\n",
      "|   64|\n",
      "|    1|\n",
      "|   41|\n",
      "|   30|\n",
      "|    6|\n",
      "|    4|\n",
      "|  230|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#manually assign schema of to the json_data by loading and select only count column and store the result in a \n",
    "from pyspark.sql.types import StructField,StructType,LongType,StringType\n",
    "from pyspark.sql.functions import col\n",
    "schema=StructType([StructField(\"DEST_COUNTRY_NAME\",StringType(),True),StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),StructField(\"count\",LongType(),True)])\n",
    "json_data=spark.read.format(\"json\").schema(schema).load(\"2015-summary.json\")\n",
    "json_data.show()\n",
    "a= json_data.select(col(\"count\"))\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "705cdf7a-34fa-4580-81ed-ad0d654ab68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n"
     ]
    }
   ],
   "source": [
    "#access only columns of json data and store in json_data and print the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\").columns\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b877065-8ba4-48a2-87e1-407210f87b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable and print first record as a row type\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab0351a5-bf8e-4e24-b774-69d3ef9a4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#create a row of values manually and show the result of 2nd index\n",
    "from pyspark.sql import Row\n",
    "row_data=Row(\"1\",None,1,False)\n",
    "print(row_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3af9ae52-3fd8-4180-bcde-caadc7fa7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the json data into json_data variable and create temp table df_table\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e76b5c7f-9c62-4434-82b3-3906dd19faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+\n",
      "|first|   second|third|\n",
      "+-----+---------+-----+\n",
      "|   hi|sushmitha|    1|\n",
      "|hello|    dolly|    2|\n",
      "+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in manual_schema store schema(string,string,int) and df_data store random data \n",
    "#and create the dataframe in new_df variable with df_data and manual_schema and print the result of new_df\n",
    "from pyspark.sql.types import StructField,StructType,StringType,LongType\n",
    "from pyspark.sql import Row\n",
    "manual_schema=StructType([StructField(\"first\",StringType(),True),StructField(\"second\",StringType(),True),StructField(\"third\",LongType(),True)])\n",
    "df_data=[Row(\"hi\",\"sushmitha\",1),Row(\"hello\",\"dolly\",2)]\n",
    "new_df=spark.createDataFrame(df_data,manual_schema)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dda9e43-f5ba-4c0c-9e65-3d734d6e9f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select only \"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\" and show the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.show()\n",
    "json_data.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e9672f9-d629-456b-b2a8-74197119b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+\n",
      "|    United States|    United States|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and using col,column,expr select the column DEST_COUNTRY_NAME and show the result of 2 records\n",
    "from pyspark.sql.functions import expr,col,column\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(col(\"DEST_COUNTRY_NAME\"),expr(\"DEST_COUNTRY_NAME\"),column(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "json_data.select(col(\"DEST_COUNTRY_NAME\"),\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa85b514-b1cf-4691-a59e-a2ad286c1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select the column DEST_COUNTRY_NAME as destination with expr and show 2 records\n",
    "from pyspark.sql.functions import expr\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2738625f-2c88-42c7-b40b-4266422d9c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| destination1|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "|United States|\n",
      "|        Egypt|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select the column DEST_COUNTRY_NAME as dest with expr, again rename to destination1 and show 5 records\n",
    "from pyspark.sql.functions import expr\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.select(expr(\"DEST_COUNTRY_NAME AS dest\").alias(\"destination1\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddee48b6-3994-4e09-aba7-e2f5aed74201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|         dest|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select the column DEST_COUNTRY_NAME as dest, DEST_COUNTRY_NAME with selectExpr, show 2 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.selectExpr(\"DEST_COUNTRY_NAME as dest\",\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42330e99-8401-4218-8b0a-03a5c5b3ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withincountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select all columns, if DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME store in withincountry and show 5 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.selectExpr(\"*\",\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withincountry\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "211f9634-fb72-40f2-9270-53a53b290d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and select avg of count,count of distinct DEST_COUNTRY_NAME\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.selectExpr(\"avg(count)\",\"count(distinct(DEST_COUNTRY_NAME))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95ad88ba-1f16-46ad-a802-646bb14d1e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select all columns of json_data and add new column as One stroing lit value of 1 and print 2 records\n",
    "from pyspark.sql.functions import lit\n",
    "json_data.select(expr(\"*\"),lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8afadb1-556c-4898-bea2-81d216f3d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|         1|\n",
      "|    United States|            Croatia|    1|         1|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|     false|\n",
      "|    United States|            Croatia|    1|     false|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|  destination|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|United States|\n",
      "|    United States|            Croatia|    1|United States|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+-------------------+-----+\n",
      "|  destination|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#a variable contains data of adding new column name as new_column contains value of literal 1 and show 2 records\n",
    "#b variable contains data of adding new column name as new_column contains value if DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME and show 2 records\n",
    "#c variable contains data of adding new column name as destination contains value of DEST_COUNTRY_NAME and show 2 records\n",
    "#d variable contains data of renaming column DEST_COUNTRY_NAME to new_column destination and show 2 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.withColumn(\"new_column\",lit(1))\n",
    "a.show(2)\n",
    "b=json_data.withColumn(\"new_column\",expr(\"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME\"))\n",
    "b.show(2)\n",
    "c=json_data.withColumn(\"destination\",expr(\"DEST_COUNTRY_NAME\"))\n",
    "c.show(2)\n",
    "c.columns\n",
    "d=json_data.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"destination\")\n",
    "d.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad9b03a8-f825-4297-bd94-aabaddcc883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------+----------+\n",
      "|This Long Column-name|new_column|\n",
      "+---------------------+----------+\n",
      "|              Romania|   Romania|\n",
      "|              Croatia|   Croatia|\n",
      "+---------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "['This Long Column-name']\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#a variable contains data of adding new column name as \"This Long Column-name\" contains value of ORIGIN_COUNTRY_NAME and show 2 records\n",
    "#b variable contains data of selecting new column name as \"This Long Column-name\" and selecting \"This Long Column-name\" as new_column and show 2 records\n",
    "#c variable contains data of selecting new column name as \"This Long Column-name\" and columns and print the result\n",
    "from pyspark.sql.functions import expr\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.withColumn(\"This Long Column-name\",expr(\"ORIGIN_COUNTRY_NAME\"))\n",
    "a.show(2)\n",
    "b=a.selectExpr(\"`This Long Column-name`\",\"`This Long Column-name` as new_column\")\n",
    "b.show(2)\n",
    "c=a.select(expr(\"`This Long Column-name`\")).columns\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ecf2f26-6d13-4b24-b9a0-cccbb27bc3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and delete the column DEST_COUNTRY_NAME and show 2 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.drop(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "517d8024-844f-42cc-877e-ab1f2cdeef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True), StructField('new_column', LongType(), True)])\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|        15|\n",
      "|    United States|            Croatia|    1|         1|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True), StructField('new_column', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and \n",
    "#a variable contains data of adding new column name as \"new_column\" contains value of \"count\" column and show 2 records,schema\n",
    "#b variable contains data of a and selecting new_column column name and change the type to int and show the schema\n",
    "\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.withColumn(\"new_column\",expr(\"count\"))\n",
    "print(a.schema)\n",
    "a.show(2)\n",
    "b=a.withColumn(\"new_column\",col(\"new_column\").cast(\"int\"))\n",
    "print(b.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89c1c789-d802-45ce-82d2-406d5f06b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "|            Malta|      United States|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and \n",
    "#a variable contains data of count<2 with where condition and show 4 records\n",
    "#b variable contains data of count<2 with filter condition and show 4 records\n",
    "#c variable contains data of count<2,ORIGIN_COUNTRY_NAME!=Croatia with where condition and show 4 records\n",
    "#d variable contains data of \"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\" distinct and show count of records\n",
    "#e variable contains data of \"DEST_COUNTRY_NAME\" distinct and show count of records\n",
    "from pyspark.sql.functions import col\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.where(\"count<2\")\n",
    "a.show(4)\n",
    "b=json_data.filter(col(\"count\")<2)\n",
    "b.show(4)\n",
    "c=json_data.where(\"count<2\").where(col(\"ORIGIN_COUNTRY_NAME\")!=\"Croatia\")\n",
    "c.show(4)\n",
    "d=json_data.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "print(d)\n",
    "e=json_data.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d1577d4-f79b-45c5-ae9a-e65240f219f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable, with sample split the data into 50% with constant 5 with replacement as false and count the result\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "seed=5\n",
    "withReplacement=False\n",
    "fraction=0.5\n",
    "json_data.sample(withReplacement,fraction,seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "beb79bfb-1221-41ca-9fe6-beef4b8d353b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the json data in json_data variable and split the data into 25%,75% with seed 5 and check count if its true or false\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "seed=5\n",
    "random_split=json_data.randomSplit([0.25,0.75],seed)\n",
    "random_split[0].count()>random_split[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e22096a-0714-4015-97ab-8197a284ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])\n",
      "ParallelCollectionRDD[1236] at readRDDFromFile at PythonRDD.scala:289\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|             sush|              asara|    2|\n",
      "|            dolly|              asara|    4|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Liberia|      United States|    2|\n",
      "|          Hungary|      United States|    2|\n",
      "|    United States|            Vietnam|    2|\n",
      "|         Malaysia|      United States|    2|\n",
      "|          Croatia|      United States|    2|\n",
      "|    United States|            Liberia|    2|\n",
      "|    United States|              Malta|    2|\n",
      "|          Georgia|      United States|    2|\n",
      "|            Niger|      United States|    2|\n",
      "|    United States|          Indonesia|    2|\n",
      "|        Greenland|      United States|    2|\n",
      "|             sush|              asara|    2|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data into json_data variable and store the schema of json_data in schema variable\n",
    "#add new_rows contains (\"sush\",\"asara\",2),(\"dolly\",\"asara\",4)\n",
    "#parallelize the new_rows store in parallelize variable\n",
    "#create a dataframe with parallelize ,schema in new_df variable and show the result of new_df\n",
    "#json_data union new_df where count=2,dest_country_name!=origin_country_name and show the result\n",
    "from pyspark.sql import Row\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "schema=json_data.schema\n",
    "print(schema)\n",
    "new_rows=[Row(\"sush\",\"asara\",2),Row(\"dolly\",\"asara\",4)]\n",
    "parallelize=spark.sparkContext.parallelize(new_rows)\n",
    "print(parallelize)\n",
    "new_df=spark.createDataFrame(parallelize,schema)\n",
    "new_df.show()\n",
    "json_data.union(new_df).where(col(\"count\")==2).where(col(\"DEST_COUNTRY_NAME\")!=col(\"ORIGIN_COUNTRY_NAME\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5440a391-5460-4c29-aa98-e9af7927dc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#in a variable sort the result of count with sort and show 5 records\n",
    "#in b variable sort the result of count,DEST_COUNTRY_NAME with orderBy and show 5 records\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.sort(\"count\")\n",
    "a.show(5)\n",
    "b=json_data.orderBy(col(\"count\"),col(\"DEST_COUNTRY_NAME\"))\n",
    "b.show(5)\n",
    "c=json_data.orderBy(\"count\",\"DEST_COUNTRY_NAME\")\n",
    "c.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1beb2718-0fbe-4f89-9a4c-19145b08f3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "|           Angola|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable\n",
    "#sort the count values in desc with orderby in a variable and show 2 records\n",
    "#sort the count values in desc and dest_country_name in asc with orderby in b variable and show 2 records\n",
    "from pyspark.sql.functions import asc,desc,col\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "a=json_data.orderBy(expr(\"count desc\"))\n",
    "a.show(2)\n",
    "b=json_data.orderBy(col(\"DEST_COUNTRY_NAME\").asc(),col(\"count\").desc())\n",
    "b.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2ed0577-f39a-4cce-b8d4-80c5be531045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the json data in json_data variable and using limit with 5 show the result\n",
    "#in a variable sort the count desc and using limit with 5 show the result\n",
    "#in b variable repartition json_data to 5 and check how many partitioned it got\n",
    "#in c variable repartition json_data based on column ORIGIN_COUNTRY_NAME and check how many partitioned it got\n",
    "#in d variable repartition json_data based on column DEST_COUNTRY_NAME with 4 and check how many partitioned it got\n",
    "from pyspark.sql.functions import desc,col\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "json_data.limit(5).show()\n",
    "a=json_data.orderBy(col(\"count\").desc()).limit(5)\n",
    "a.show()\n",
    "json_data.rdd.getNumPartitions()\n",
    "b=json_data.repartition(5)\n",
    "b.rdd.getNumPartitions()\n",
    "c=json_data.repartition(col(\"ORIGIN_COUNTRY_NAME\"))\n",
    "c.rdd.getNumPartitions()\n",
    "d=json_data.repartition(4,col(\"DEST_COUNTRY_NAME\"))\n",
    "d.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "486dc14b-b29a-47f1-aa8c-cac84d62e754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "|United States    |Singapore          |1    |\n",
      "|United States    |Grenada            |62   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "|Moldova          |United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the json data in json_data variable and print the result with show,take,show(value,False),collect\n",
    "json_data=spark.read.format(\"json\").load(\"2015-summary.json\")\n",
    "#json_data.take(5)\n",
    "json_data.show(5)\n",
    "#json_data.collect()\n",
    "json_data.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9c425d0-da32-4782-8a08-ec46d9c4dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#create a temp table retail_temp_table and print schema of retail_data\n",
    "retail_data=spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.show(5,False)\n",
    "retail_data.createOrReplaceTempView(\"retail_temp_table\")\n",
    "retail_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6330c291-eefa-4b35-9e1c-c37b2355595e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 0.5: double]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lit and select the 5,0.5,five\n",
    "from pyspark.sql.functions import lit\n",
    "retail_data.select(lit(5),lit(\"five\"),lit(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f98d4856-53c7-408f-9be4-e0f84c0467ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from retail_data select invoiceno,description where InvoiceNo not equal to 536365 and show the result\n",
    "retail_data.where(col(\"InvoiceNo\")!=\"536365\") \\\n",
    ".select(\"InvoiceNo\",\"Description\") \\\n",
    ".show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab682775-cd98-48f5-ae06-d56ad95a5db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description   |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|536544   |DOT      |DOTCOM POSTAGE|1       |2010-12-01 14:32:00|569.77   |NULL      |United Kingdom|\n",
      "|536592   |DOT      |DOTCOM POSTAGE|1       |2010-12-01 17:06:00|607.49   |NULL      |United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description   |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|536544   |DOT      |DOTCOM POSTAGE|1       |2010-12-01 14:32:00|569.77   |NULL      |United Kingdom|\n",
      "|536592   |DOT      |DOTCOM POSTAGE|1       |2010-12-01 17:06:00|607.49   |NULL      |United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#in pricefilter variable check UnitPrice is grater tan 600\n",
    "#in descriptionfilter variable check Description col contains POSTAGE and greater than or equal to 1 with instr\n",
    "#with reatial_data check StockCode is dot with == and pricefilter or descriptionfilter and show\n",
    "#with reatial_data check StockCode is dot with isin and pricefilter or descriptionfilter and show\n",
    "from pyspark.sql.functions import instr\n",
    "retail_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"2010-12-01.csv\")\n",
    "pricefilter=col(\"UnitPrice\") > 600\n",
    "descriptionfilter=instr(col(\"Description\"),\"POSTAGE\") >= 1\n",
    "retail_data.where(col(\"StockCode\") == \"DOT\").where(pricefilter | descriptionfilter).show(5,False)\n",
    "\n",
    "retail_data.where(retail_data.StockCode.isin(\"DOT\")).where(pricefilter | descriptionfilter).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52e50045-e602-43bd-855a-b07cad9c0347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|Description   |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |isexpensive|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|536544   |DOT      |DOTCOM POSTAGE|1       |2010-12-01 14:32:00|569.77   |NULL      |United Kingdom|true       |\n",
      "|536592   |DOT      |DOTCOM POSTAGE|1       |2010-12-01 17:06:00|607.49   |NULL      |United Kingdom|true       |\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#in pricefilter variable check UnitPrice is grater tan 60\n",
    "#in stockcodefilter variable check stockcodefilter contains dot init\n",
    "#in descriptionfilter variable check Description col contains POSTAGE and greater than or equal to 1 with instr\n",
    "#with reatial_data add new column isexpensive where  descriptionfilter and (pricefilter or stockcodefilter) in checking_filters variable\n",
    "#and show if isexpensive is true\n",
    "from pyspark.sql.functions import instr\n",
    "retail_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.show(5,False)\n",
    "descriptionfilter=instr(col(\"Description\"),\"POSTAGE\")>=1\n",
    "pricefilter=col(\"UnitPrice\")>600\n",
    "stockcodefilter=col(\"StockCode\").isin(\"DOT\")\n",
    "checking_filters=retail_data.withColumn(\"isexpensive\",descriptionfilter & (pricefilter | stockcodefilter))\n",
    "checking_filters.where(\"isexpensive\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "221321a4-5a95-44fa-bbf0-f46a99d0c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|isexpensive|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|       true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|       true|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and print the result with show and \n",
    "#add new column isexpensive where not(unitprice) is less than or equal to 250 and show if isexpensive is true\n",
    "retail_data=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.show(5,False)\n",
    "retail_data.withColumn(\"isexpensive\",expr(\"NOT UnitPrice<=250\")).where(\"isexpensive\").show()\n",
    "retail_data.where(col(\"Description\").eqNullSafe(\"hello\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6924b134-0a1f-4079-9476-f619281009b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and check col description value contains hello with eqnullsafe\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.where(col(\"Description\").eqNullSafe(\"hello\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b46c3eef-8180-4a35-90be-f13c0fde5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|            actual|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+------------------+\n",
      "|CustomerID|            actual|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#using pow function ((Quantity*UnitPrice) ^2 )+5 store in fabricated_quantity variable\n",
    "#and show the result with just select statement of 2 records of columns CustomerID and its fabricated_quantity as \"actual\"\n",
    "#and show the result with just selectexpr statement of 2 records of columns CustomerID and its fabricated_quantity as \"actual\"\n",
    "\n",
    "from pyspark.sql.functions import expr,pow\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "fabricated_quantity=pow(col(\"Quantity\")*col(\"UnitPrice\"),2)+5\n",
    "retail_data.select(col(\"CustomerID\"),fabricated_quantity.alias(\"actual\")).show(2)\n",
    "retail_data.selectExpr(\"CustomerID\",\"POWER((Quantity * UnitPrice),2)+5 as actual\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b171acf9-5692-4700-9759-dfa6ebd6cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#with round(lit(2.5)),bround(lit(2.5)) check what does the output\n",
    "from pyspark.sql.functions import lit,round,bround\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.select(round(lit(2.5)),bround(lit(2.5))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73cf022b-37fe-4fab-995d-d85eb684936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|corr(UnitPrice, Quantity)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835552|\n",
      "+-------------------------+\n",
      "\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                NULL| 8.627413127413128| 4.151946589446603|15661.388719512195|          NULL|\n",
      "| stddev|72.89447869788873|17407.897548583845|                NULL|26.371821677029203|15.638659854603892|1854.4496996893627|          NULL|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#with corr between columns UnitPrice,Quantity check how much its linear relationship \n",
    "#with describe function check the retail_data of count,min,max,mean,stddev\n",
    "from pyspark.sql.functions import corr\n",
    "retail_data.select(corr(col(\"Quantity\"),col(\"UnitPrice\"))).show()\n",
    "retail_data.select(corr(col(\"UnitPrice\"),col(\"Quantity\"))).show()\n",
    "retail_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "912a1a1b-8325-441b-a56f-c49fcc5364a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             21259|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21894|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21452|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22728|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             21889|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[22086, 21705, 72...|[200, 128, 23, 50...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#colname variable contains unitprice,quantileprobs variable contains list of 0.5 value, relerror contains 0.05 value\n",
    "#check .stat.approxQuantile with 3 varibales with retail_data\n",
    "#check .stat.crosstab with \"StockCode\",\"Quantity\" varibales with retail_data\n",
    "#check .stat.freqItems with \"StockCode\",\"Quantity\" varibales with retail_data\n",
    "#with monotonically_increasing_id select and show 2 records\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "colName=\"UnitPrice\"\n",
    "quantileProbs=[0.5,0.25]\n",
    "relError=0.05\n",
    "retail_data.stat.approxQuantile(colName,quantileProbs,relError)\n",
    "retail_data.stat.crosstab(\"StockCode\",\"Quantity\").show(5)\n",
    "retail_data.stat.freqItems([\"StockCode\",\"Quantity\"]).show(5)\n",
    "retail_data.select(monotonically_increasing_id()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "316d079d-69b8-428c-94ef-b1952da67029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|lower(Description)                 |upper(Description)                 |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|white hanging heart t-light holder |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|white metal lantern                |WHITE METAL LANTERN                |\n",
      "|cream cupid hearts coat hanger     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|knitted union flag hot water bottle|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|red woolly hottie white heart.     |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load the csv retail data in retail_data variable and \n",
    "#use function initcap,upper,lower for description variable and show result\n",
    "from pyspark.sql.functions import initcap,lower,upper\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.select(initcap(col(\"Description\"))).show(5,False)\n",
    "retail_data.select(lower(\"Description\"),upper(col(\"Description\"))).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b324f2f6-50ab-4651-8134-5f7b3c097505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|ltrim(    HELLO    )|rtrim(    HELLO    )|trim(    HELLO    )|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|           HELLO    |               HELLO|              HELLO|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+----------------------+\n",
      "|lpad( pHELLO, 13, 2)|rpad( p HELLO , 10, 6)|\n",
      "+--------------------+----------------------+\n",
      "|       222222 pHELLO|             p HELLO 6|\n",
      "+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using ltrim,rtrim,rpad,lpad,trim select (\"    HELLO    \") and check the result\n",
    "from pyspark.sql.functions import ltrim,rtrim,rpad,lpad,trim\n",
    "retail_data.select(ltrim(lit(\"    HELLO    \")),rtrim(lit(\"    HELLO    \")),trim(lit(\"    HELLO    \"))).show(1)\n",
    "retail_data.select(lpad(lit(\" pHELLO\"),13,\"2\"),rpad(lit(\" p HELLO \"),10,\"6\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5604ac75-513c-4bb0-873f-dc14553d87ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|regexp_replace(Description, BLACK|WHITE|RED|GREEN|BLUE, sush, 1)|\n",
      "+----------------------------------------------------------------+\n",
      "|sush HANGING HEART T-LIGHT HOLDER                               |\n",
      "|sush METAL LANTERN                                              |\n",
      "|CREAM CUPID HEARTS COAT HANGER                                  |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE                             |\n",
      "|sush WOOLLY HOTTIE sush HEART.                                  |\n",
      "+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------------------------+\n",
      "|translate(Description, HANGING, 1234)|\n",
      "+-------------------------------------+\n",
      "|W1TE 123434 1E2RT T-L41T 1OLDER      |\n",
      "|W1TE MET2L L23TER3                   |\n",
      "|CRE2M CUPD 1E2RTS CO2T 1234ER        |\n",
      "|K3TTED U3O3 FL24 1OT W2TER BOTTLE    |\n",
      "|RED WOOLLY 1OTTE W1TE 1E2RT.         |\n",
      "+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using regexp_replace wherever col description contains BLACK|WHITE|RED|GREEN|BLUE then replaced with sush\n",
    "#using translate wherever hanging comes replace with 1234\n",
    "from pyspark.sql.functions import regexp_replace,translate\n",
    "regex_string=\"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "retail_data.select(regexp_replace(col(\"Description\"),regex_string,\"sush\")).show(5,False)\n",
    "retail_data.select(translate(col(\"Description\"),\"HANGING\",\"1234\")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc4cafd3-f57d-4528-8592-23de0a5dc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+--------------------+\n",
      "|regexp_extract(Description, (BLACK|WHITE|RED|CREAM|BLUE), 1)|         Description|\n",
      "+------------------------------------------------------------+--------------------+\n",
      "|                                                       WHITE|WHITE HANGING HEA...|\n",
      "|                                                       WHITE| WHITE METAL LANTERN|\n",
      "|                                                       CREAM|CREAM CUPID HEART...|\n",
      "|                                                            |KNITTED UNION FLA...|\n",
      "|                                                         RED|RED WOOLLY HOTTIE...|\n",
      "+------------------------------------------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using regexp_extract wherever col description contains BLACK|WHITE|RED|GREEN|BLUE then extract the first\n",
    "# For each row, it returns the extracted color (if any) along with the original \"Description\" value and then displays the first 5 result\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "regexp_string=\"(BLACK|WHITE|RED|CREAM|BLUE)\"\n",
    "retail_data.select(regexp_extract(col(\"Description\"),regexp_string,1),col(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61092f9d-e67c-4c46-850c-0a8a36438e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#contains_black variable check col(Description) contains black,contains_white variable check col(Description) contains white\n",
    "#add new column to retail_data haswhiteorblack with or condition check it contains black or white and that is true and select only description\n",
    "from pyspark.sql.functions import instr\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "contains_black=instr(col(\"Description\"),\"BLACK\")>=1\n",
    "contains_white=instr(col(\"Description\"),\"WHITE\")>=1\n",
    "retail_data.withColumn(\"haswhiteorblack\",contains_black | contains_white).where(\"haswhiteorblack\").select(col(\"Description\")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fed4690b-ccb4-4a4f-b8e5-d5574cfd59f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS is_black'>, Column<'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS is_white'>, Column<'CAST(locate(RED, Description, 1) AS BOOLEAN) AS is_red'>, Column<'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS is_blue'>, Column<'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS is_green'>]\n",
      "+--------+--------+------+-------+--------+----------------------------------+\n",
      "|is_black|is_white|is_red|is_blue|is_green|Description                       |\n",
      "+--------+--------+------+-------+--------+----------------------------------+\n",
      "|false   |true    |false |false  |false   |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|false   |true    |false |false  |false   |WHITE METAL LANTERN               |\n",
      "|false   |true    |true  |false  |false   |RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+--------+--------+------+-------+--------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a list simplecolors=[\"black\",\"white\",\"red\",\"blue\",\"green\"]\n",
    "#selectedcolumns varibale is an empty list\n",
    "#create a function colo_locator \n",
    "#you have a DataFrame named retail_data with a column Description containing text descriptions of retail items. \n",
    "#You want to create new boolean columns indicating the presence of specific colors in the Description column with is_red,is_black etc\n",
    "#After that, you wish to filter and display the rows where the Description contains either \"white\" or \"red\".\n",
    "from pyspark.sql.functions import locate\n",
    "simplecolors=[\"black\",\"white\",\"red\",\"blue\",\"green\"]\n",
    "selectedcolumns=[]\n",
    "\n",
    "def color_locator(column,color_string):\n",
    "    return locate(color_string.upper(),column).cast(\"boolean\").alias(\"is_\"+color_string)\n",
    "for i in simplecolors:\n",
    "    a=color_locator(retail_data.Description,i)\n",
    "    selectedcolumns.append(a)\n",
    "print(selectedcolumns)\n",
    "selectedcolumns.append(col(\"Description\"))\n",
    "retail_data.select(*selectedcolumns).where(expr(\"is_white or is_red\")).show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45041814-df5c-4a24-94a5-2d4856a2539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+--------------------------+\n",
      "|id |today     |now                       |\n",
      "+---+----------+--------------------------+\n",
      "|0  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|1  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|2  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|3  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|4  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|5  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|6  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|7  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|8  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "|9  |2025-04-01|2025-04-01 10:28:55.597576|\n",
      "+---+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe datadf of range(10) and adding two columns today column contains current_date and now column contains current_timestamp\n",
    "#create a temp table \"datatable\" with datadf\n",
    "#print the schema of datadf\n",
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "datadf=spark.range(10).withColumn(\"today\",current_date()).withColumn(\"now\",current_timestamp())\n",
    "datadf.createOrReplaceTempView(\"datatable\")\n",
    "datadf.printSchema()\n",
    "\n",
    "datadf.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80001c89-fed8-4d37-ad24-6c666db62f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_add(today, 5)|date_sub(today, 5)|\n",
      "+------------------+------------------+\n",
      "|2025-04-06        |2025-03-27        |\n",
      "|2025-04-06        |2025-03-27        |\n",
      "|2025-04-06        |2025-03-27        |\n",
      "|2025-04-06        |2025-03-27        |\n",
      "|2025-04-06        |2025-03-27        |\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import date_add,date_sub and with existing datadf dataframe ad 5 days to today column and subtract 5 days to today column and show\n",
    "from pyspark.sql.functions import date_add,date_sub \n",
    "datadf.select(date_add(col(\"today\"),5),date_sub(col(\"today\"),5)).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "385a6d6a-7a1b-412e-9af1-cf12286783f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------------------------------+\n",
      "|months_between(start_date, end_date, true)|\n",
      "+------------------------------------------+\n",
      "|                              -16.67741935|\n",
      "|                              -16.67741935|\n",
      "|                              -16.67741935|\n",
      "|                              -16.67741935|\n",
      "|                              -16.67741935|\n",
      "+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import 3 functions datediff,months_between,to_date\n",
    "#with existing datadiff dataframe add new column week_age contains data of subtract 7 days of today column and checkdiff with datediff and show diff\n",
    "#with existing datadiff dataframe lit(\"2016-01-01\"),lit(\"2017-05-22\") convert the 2 lit into date with to_date and start_date and end_date columns\n",
    "#select months_between diff of 2 columns of start_date and end_date columns\n",
    "from pyspark.sql.functions import datediff,months_between,to_date\n",
    "datadf.withColumn(\"week_ago\",date_sub(col(\"today\"),7)).select(datediff(col(\"week_ago\"),col(\"today\"))).show(1)\n",
    "datadf.select(to_date(lit(\"2016-01-01\")).alias(\"start_date\"),to_date(lit(\"2017-05-22\")).alias(\"end_date\")) \\\n",
    ".select(months_between(col(\"start_date\"),col(\"end_date\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "656c20ce-dc05-441f-aae6-ba7ccdcb832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  0|2000-10-16|\n",
      "+---+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2000-10-16|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|to_date(2000-16-10)|to_date(2000-10-16)|\n",
      "+-------------------+-------------------+\n",
      "|               NULL|         2000-10-16|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe with range(5) and add new column \"date\" contains value of \"2000-10-16\" and while select convert into str to date with to_date\n",
    "#with existing datadf dataframe select \"2000-10-16\",\"2000-16-10\" by converting into date with to_date\n",
    "datadf1=spark.range(5).withColumn(\"date\",lit(\"2000-10-16\"))\n",
    "datadf1.show(1)\n",
    "datadf=spark.range(5).withColumn(\"date\",lit(\"2000-10-16\")).select(to_date(col(\"date\")))\n",
    "datadf.show(1)\n",
    "datadf.select(to_date(lit(\"2000-16-10\")),to_date(lit(\"2000-10-16\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4816ffd6-e9b6-4e57-82af-436ffa2d5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     first|    second|\n",
      "+----------+----------+\n",
      "|2000-10-16|2000-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+-------------------+--------------------+\n",
      "|to_timestamp(first)|to_timestamp(second)|\n",
      "+-------------------+--------------------+\n",
      "|2000-10-16 00:00:00| 2000-12-20 00:00:00|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "+----------+----------+\n",
      "|     first|    second|\n",
      "+----------+----------+\n",
      "|2000-10-16|2000-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import to_timestamp function \n",
    "#in dateFormat variable store the format 'yyyy-dd-MM'\n",
    "#create dataframe range(1) and\n",
    "#selectint 2 columns with to_date 1 is \"first\" contains value \"2000-16-10\" and \"second\" contains value \"2000-20-12\"\n",
    "#with to_timestamp select first and second columns\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "dateFormat='yyyy-dd-MM'\n",
    "cleandatediff=spark.range(1).select(to_date(lit(\"2000-16-10\"),dateFormat).alias(\"first\"),to_date(lit(\"2000-20-12\"),dateFormat).alias(\"second\"))\n",
    "cleandatediff.show()\n",
    "\n",
    "cleandatediff.select(to_timestamp(col(\"first\")),to_timestamp(col(\"second\"))).show(1)\n",
    "cleandatediff.filter(col(\"first\")<col(\"second\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10fb9234-6bd5-4a5b-911f-44174da7458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|complex                                     |InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "+--------------------------------------------+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import struct, using struct select \"Description\",\"InvoiceNo\" columns as \"complex\"\n",
    "#create a table complexDF_table from the complexDF dataframe\n",
    "from pyspark.sql.functions import struct\n",
    "retail_data=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"2010-12-01.csv\")\n",
    "retail_data.show(1)\n",
    "#retail_data.selectExpr(\"(Description,InvoiceNo) as complex\",\"*\").show(1,False)\n",
    "complexDF=retail_data.select(struct(\"Description\",\"InvoiceNo\").alias(\"complex\"),\"*\")\n",
    "complexDF.show(1,False)\n",
    "complexDF.createOrReplaceTempView(\"complexDF_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b5c53b1f-7f80-448b-a372-af1f94c1a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|complex                                     |\n",
      "+--------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365}|\n",
      "+--------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+\n",
      "|complex.InvoiceNo|\n",
      "+-----------------+\n",
      "|536365           |\n",
      "+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+---------+\n",
      "|Description                       |InvoiceNo|\n",
      "+----------------------------------+---------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |\n",
      "+----------------------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from complexDF dataframe select col complex,col complex of description,col complex of InvoiceNo with getfield\n",
    "#select complex data in different columns like description and invoiceno\n",
    "complexDF.select(\"complex\").show(1,False)\n",
    "complexDF.select(\"complex.Description\").show(1,False)\n",
    "complexDF.select(col(\"complex\").getField(\"InvoiceNo\")).show(1,False)\n",
    "complexDF.select(col(\"complex.*\")).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8f4068c0-74a4-4689-ac72-b1a8bb7cca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+\n",
      "|Description                       |split(Description,  , -1)               |\n",
      "+----------------------------------+----------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "+----------------------------------+----------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------------+----------------------------------------+\n",
      "|Description                       |array_col                               |\n",
      "+----------------------------------+----------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "+----------------------------------+----------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the spli function, with retail_data select description and \n",
    "#for every spac split the description and rename new column as array_col\n",
    "from pyspark.sql.functions import split\n",
    "retail_data.select(col(\"Description\"),split(col(\"Description\"),\" \")).show(1,False)\n",
    "retail_data.select(col(\"Description\"),split(col(\"Description\"),\" \").alias(\"array_col\")).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "24ce1d2e-3880-48f2-9895-9d49745033e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-------------------------------+\n",
      "|description                       |size(split(description,  , -1))|\n",
      "+----------------------------------+-------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|5                              |\n",
      "|WHITE METAL LANTERN               |3                              |\n",
      "+----------------------------------+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the size function, with retail_data select description and \n",
    "#check the size after spliting the col description with \" \" \n",
    "from pyspark.sql.functions import size\n",
    "retail_data.select(col(\"description\"),size(split(col(\"description\"),\" \"))).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9dfbae64-1959-40cc-ba6b-0fc917a1f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|true                                            |\n",
      "+------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the array_contains function, with retail_data select description and \n",
    "#check the array_contains has white in description column\n",
    "from pyspark.sql.functions import array_contains\n",
    "retail_data.select(array_contains(split(col(\"Description\"),\" \"),\"WHITE\")).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1fb2f02b-54de-4612-93e3-3a6f780c6b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the explode function, with retail_data add new column \"splitted\" contains splitting of description with \" \"\n",
    "#and add another new column \"exploded\" contains explod of splitted column\n",
    "#and select columns \"Description\", \"InvoiceNo\", \"exploded\"\n",
    "from pyspark.sql.functions import explode\n",
    "retail_data.withColumn(\"splitted\",split(col(\"Description\"),\" \")) \\\n",
    ".withColumn(\"exploded\",explode(col(\"splitted\"))) \\\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fd6c4e54-c3ca-4051-9b33-e600ee2f50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|complex_map                                   |\n",
      "+----------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365}|\n",
      "|{WHITE METAL LANTERN -> 536365}               |\n",
      "+----------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            NULL|\n",
      "|                          536365|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "|                            NULL|\n",
      "+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the create_map function, with retail_data select create_map of Description and InvoiceNo\n",
    "#rename it as complex_map\n",
    "#do the same as above but now extra select like new_col[some value]\n",
    "from pyspark.sql.functions import create_map\n",
    "retail_data.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complex_map\")).show(2,False)\n",
    "retail_data.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c97743c3-e013-436c-b1c3-9f375932bf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfexampledf=spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value**3\n",
    "power3(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4b74cf65-968e-47aa-9068-46fb8f3bcef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf=udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce425f3c-8945-4118-82f0-fb803e304538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
